{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the resource article Chapelle, et al., the following delineates the data processing steps required before implementing the algorithms for the final project. There are several steps:\n",
    "\n",
    "- selection of features to be included in the model\n",
    "- one-hot encoding\n",
    "- stashing of under-represented values at a determined threshold\n",
    "- feature hashing to regulate the size of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assign global variables\n",
    "# ZONE=us-central1-a\n",
    "# PROJECT_ID=w261projects\n",
    "# CLUSTER_NAME=w261finalproject\n",
    "# BUCKET_NAME=danielalvarez_w261projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign global variables for input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign input (read from) and output (write to) files    \n",
    "#INPUT_FILE = 'gs://261_projectdata/261project_data/df.parquet'\n",
    "INPUT_FILE = 'data/df.parquet'\n",
    "\n",
    "#OUT_FILE = 'gs://261_projectdata/261project_data/df_ohe_10k.parquet'\n",
    "OUTPUT_FILE = 'data/df_ohe_300.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To run, execute in bash shell\n",
    "# python submit_job_to_bigger_cluster.py --project_id=${PROJECT_ID} --zone=central1-a --cluster_name=w261finalproject --gcs_bucket=${BUCKET_NAME} --key_file=$HOME/MIDS/w261.json --create_new_cluster --pyspark_file=data_processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.rdd.compress', 'True')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.name', 'ohe')\n",
      "('spark.driver.port', '44775')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.app.id', 'local-1576350682767')\n",
      "('spark.driver.host', 'docker.w261')\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import time\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit, when, col, approx_count_distinct\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, OneHotEncoderEstimator, VectorAssembler, FeatureHasher\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "app_name = \"ohe\"\n",
    "\n",
    "#set session configuration\n",
    "conf=SparkConf()\n",
    "conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "#Create Spark session\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "spark\n",
    "#print session configuration\n",
    "for object in sc.getConf().getAll():\n",
    "    print(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data tranformation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a function to filter each feature to contain unique values each with a threshold number of counts (filter out low occurrence values). For features with over a threshold number of unique values, we decide to create a stash variable that comprises the least represented values for each original feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this variable takes the minimum category occurence count as the threshold\n",
    "#CASE 1 - 10_000\n",
    "#CASE 2 - 3_000\n",
    "#THRESHOLD = 10_000\n",
    "THRESHOLD = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the columns to be transformed and load parquet file as a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total number of Rows = 46048\n"
     ]
    }
   ],
   "source": [
    "# Column names which will be transformed\n",
    "my_cats = ['cat14', 'cat15', 'cat16', 'cat17',\n",
    "           'cat18', 'cat19', 'cat20', 'cat21', 'cat22',\n",
    "           'cat23', 'cat24', 'cat25', 'cat26', 'cat27',\n",
    "           'cat28', 'cat29', 'cat30', 'cat31', 'cat32',\n",
    "           'cat33', 'cat34', 'cat35', 'cat36', 'cat37',\n",
    "           'cat38', 'cat39']\n",
    "\n",
    "my_nums = ['n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7',\n",
    "           'n8', 'n9', 'n10', 'n11', 'n12', 'n13']\n",
    "\n",
    "#load file as dataframe\n",
    "df_pq = spark.read.load(INPUT_FILE)\n",
    "print(f' Total number of Rows = {df_pq.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming Numerical column.. n1\n",
      "... completed job in 2.9599125385284424 seconds\n",
      "Transforming Numerical column.. n2\n",
      "... completed job in 2.854212760925293 seconds\n",
      "Transforming Numerical column.. n3\n",
      "... completed job in 2.741354465484619 seconds\n",
      "Transforming Numerical column.. n4\n",
      "... completed job in 2.763197422027588 seconds\n",
      "Transforming Numerical column.. n5\n",
      "... completed job in 2.775784730911255 seconds\n",
      "Transforming Numerical column.. n6\n",
      "... completed job in 2.8138625621795654 seconds\n",
      "Transforming Numerical column.. n7\n",
      "... completed job in 2.7581288814544678 seconds\n",
      "Transforming Numerical column.. n8\n",
      "... completed job in 2.8379647731781006 seconds\n",
      "Transforming Numerical column.. n9\n",
      "... completed job in 2.8326516151428223 seconds\n",
      "Transforming Numerical column.. n10\n",
      "... completed job in 2.7632012367248535 seconds\n",
      "Transforming Numerical column.. n11\n",
      "... completed job in 2.7656874656677246 seconds\n",
      "Transforming Numerical column.. n12\n",
      "... completed job in 2.6963412761688232 seconds\n",
      "Transforming Numerical column.. n13\n",
      "... completed job in 2.7217812538146973 seconds\n",
      "total time taken = 36.28408098220825\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Transforming Categorical column.. cat14\n",
      "... completed job in 2.7276530265808105 seconds\n",
      "Transforming Categorical column.. cat15\n",
      "... completed job in 2.775857925415039 seconds\n",
      "Transforming Categorical column.. cat16\n",
      "... completed job in 2.7433810234069824 seconds\n",
      "Transforming Categorical column.. cat17\n",
      "... completed job in 2.7468700408935547 seconds\n",
      "Transforming Categorical column.. cat18\n",
      "... completed job in 2.6662471294403076 seconds\n",
      "Transforming Categorical column.. cat19\n",
      "... completed job in 2.66642165184021 seconds\n",
      "Transforming Categorical column.. cat20\n",
      "... completed job in 2.732267141342163 seconds\n",
      "Transforming Categorical column.. cat21\n",
      "... completed job in 2.7341363430023193 seconds\n",
      "Transforming Categorical column.. cat22\n",
      "... completed job in 2.712188959121704 seconds\n",
      "Transforming Categorical column.. cat23\n",
      "... completed job in 2.760794162750244 seconds\n",
      "Transforming Categorical column.. cat24\n",
      "... completed job in 2.723400354385376 seconds\n",
      "Transforming Categorical column.. cat25\n",
      "... completed job in 2.7581284046173096 seconds\n",
      "Transforming Categorical column.. cat26\n",
      "... completed job in 2.739642858505249 seconds\n",
      "Transforming Categorical column.. cat27\n",
      "... completed job in 2.699906349182129 seconds\n",
      "Transforming Categorical column.. cat28\n",
      "... completed job in 2.743792772293091 seconds\n",
      "Transforming Categorical column.. cat29\n",
      "... completed job in 2.734553813934326 seconds\n",
      "Transforming Categorical column.. cat30\n",
      "... completed job in 2.6631343364715576 seconds\n",
      "Transforming Categorical column.. cat31\n",
      "... completed job in 2.757014513015747 seconds\n",
      "Transforming Categorical column.. cat32\n",
      "... completed job in 2.7547218799591064 seconds\n",
      "Transforming Categorical column.. cat33\n",
      "... completed job in 2.720555305480957 seconds\n",
      "Transforming Categorical column.. cat34\n",
      "... completed job in 2.7695834636688232 seconds\n",
      "Transforming Categorical column.. cat35\n",
      "... completed job in 2.657374620437622 seconds\n",
      "Transforming Categorical column.. cat36\n",
      "... completed job in 2.7037084102630615 seconds\n",
      "Transforming Categorical column.. cat37\n",
      "... completed job in 2.7723584175109863 seconds\n",
      "Transforming Categorical column.. cat38\n",
      "... completed job in 2.783008575439453 seconds\n",
      "Transforming Categorical column.. cat39\n",
      "... completed job in 2.816755771636963 seconds\n",
      "total time taken = 71.06345725059509\n",
      "Running pipeline to create sparse vectors.. \n",
      "... completed job in 5.268675804138184 seconds\n"
     ]
    }
   ],
   "source": [
    "def transform_str_col(df, cat_name):\n",
    "    \"\"\" Transform the string 'cat_name' column based on the THRESHOLD\n",
    "        for occurence count.\n",
    "    Input -     Dataframe and the column name to be transformed\n",
    "    Output -    Return dataframe with transformed column \n",
    "                where low occurence variables --> 'stash_$col_name'\n",
    "    \"\"\"\n",
    "    df_uniq_counts = df.groupBy(cat_name).count()\n",
    "    \n",
    "    #get values that occur above the threshold and broadcast it\n",
    "    keep_vars = sc.broadcast(df_uniq_counts.filter(df_uniq_counts['count'] > THRESHOLD)\\\n",
    "                            .select(df_uniq_counts[cat_name])\\\n",
    "                            .rdd.flatMap(lambda x: x).collect())\n",
    "    \n",
    "    #broadcast the value to replace the low occurance values\n",
    "    replace_val = sc.broadcast('stash_' + str(cat_name))\n",
    "\n",
    "\n",
    "    \n",
    "    #name the new column\n",
    "    cat_t = str(cat_name) + '_t'\n",
    "    \n",
    "    df = df.withColumn(cat_t, when(col(cat_name).isin(keep_vars.value), col(cat_name)).otherwise(lit(replace_val.value)))\n",
    "    df = df.drop(cat_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def transform_num_col(df, cat_name):\n",
    "    \"\"\" Transform the numerical 'cat_name' column based on the THRESHOLD\n",
    "        for occurence count.\n",
    "    Input -   Dataframe and the column name to be transformed\n",
    "    Output -  Return dataframe with transformed column where \n",
    "                low occurence variables --> 1_000_0xx' based on my_nums dict\n",
    "    \"\"\"\n",
    "  #list of stash variables named so its easy to identify in coefficient analysis\n",
    "    my_nums = {'n1':1_000_001, 'n2':1_000_002,\n",
    "               'n3':1_000_003, 'n4':1_000_004,\n",
    "               'n5':1_000_005, 'n6':1_000_006,\n",
    "               'n7':1_000_007, 'n8':1_000_008, \n",
    "               'n9':1_000_009, 'n10':1_000_010,\n",
    "               'n11':1_000_011, 'n12':1_000_012, 'n13':1_000_012}\n",
    "    \n",
    "    df_uniq_counts = df.groupBy(cat_name).count()\n",
    "    \n",
    "    #get values that occur above the threshold and broadcast it\n",
    "    keep_vars = sc.broadcast(df_uniq_counts.filter(df_uniq_counts['count'] > THRESHOLD)\\\n",
    "                            .select(df_uniq_counts[cat_name])\\\n",
    "                            .rdd.flatMap(lambda x: x).collect())\n",
    "    \n",
    "    #broadcast the stash variable placeholder value to replace the low occurance values\n",
    "    replace_val = sc.broadcast(my_nums[cat_name])\n",
    "\n",
    "\n",
    "    \n",
    "    #name the new column\n",
    "    cat_t = str(cat_name) + '_t'\n",
    "    #if value is in the broadcasted high occurence list, then keep the value, other replace it with the placeholder\n",
    "    df = df.withColumn(cat_t, when(col(cat_name).isin(keep_vars.value), col(cat_name)).otherwise(lit(replace_val.value)))\n",
    "    df = df.drop(cat_name)\n",
    "    return df\n",
    "\n",
    "#Apply transform on numerical columns\n",
    "tot_time = 0\n",
    "for c in my_nums:\n",
    "\n",
    "    start = time.time()\n",
    "    print(f'Transforming Numerical column.. {c}')\n",
    "    df_pq = transform_num_col(df_pq, c)\n",
    "    time_taken = time.time() - start\n",
    "    print(f\"... completed job in {time_taken} seconds\")\n",
    "    tot_time += time_taken\n",
    "print(f'total time taken = {tot_time}')\n",
    "\n",
    "print('\\n')\n",
    "print('----'* 20)\n",
    "print('\\n')\n",
    "\n",
    "#Apply transform on  string columns\n",
    "tot_time = 0\n",
    "for c in my_cats:\n",
    "\n",
    "    start = time.time()\n",
    "    print(f'Transforming Categorical column.. {c}')\n",
    "    df_pq = transform_str_col(df_pq, c)\n",
    "    time_taken = time.time() - start\n",
    "    print(f\"... completed job in {time_taken} seconds\")\n",
    "    tot_time += time_taken\n",
    "print(f'total time taken = {tot_time}')\n",
    "\n",
    "df_pq.cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#specify columns to turn from string categorical variables to numerical categorical representation\n",
    "cat_cols = ['n1_t', 'n2_t', 'n3_t', 'n4_t', 'n5_t', 'n6_t', 'n7_t',\n",
    "           'n8_t', 'n9_t', 'n10_t', 'n11_t', 'n12_t', 'n13_t',\n",
    "           'cat14_t', 'cat15_t', 'cat16_t', 'cat17_t',\n",
    "           'cat18_t', 'cat19_t', 'cat20_t', 'cat21_t', 'cat22_t',\n",
    "           'cat23_t', 'cat24_t', 'cat25_t', 'cat26_t', 'cat27_t',\n",
    "           'cat28_t', 'cat29_t', 'cat30_t', 'cat31_t', 'cat32_t',\n",
    "           'cat33_t', 'cat34_t', 'cat35_t', 'cat36_t', 'cat37_t',\n",
    "           'cat38_t', 'cat39_t']\n",
    "\n",
    "cat_str_indx = ['n1_t_Indx', 'n2_t_Indx', 'n3_t_Indx', 'n4_t_Indx', 'n5_t_Indx', 'n6_t_Indx', 'n7_t_Indx',\n",
    "               'n8_t_Indx', 'n9_t_Indx', 'n10_t_Indx', 'n11_t_Indx', 'n12_t_Indx',\n",
    "               'n13_t_Indx','cat14_t_Indx', 'cat15_t_Indx', 'cat16_t_Indx', 'cat17_t_Indx',\n",
    "               'cat18_t_Indx', 'cat19_t_Indx', 'cat20_t_Indx', 'cat21_t_Indx', 'cat22_t_Indx',\n",
    "               'cat23_t_Indx', 'cat24_t_Indx', 'cat25_t_Indx', 'cat26_t_Indx', 'cat27_t_Indx',\n",
    "               'cat28_t_Indx', 'cat29_t_Indx', 'cat30_t_Indx', 'cat31_t_Indx', 'cat32_t_Indx',\n",
    "               'cat33_t_Indx', 'cat34_t_Indx', 'cat35_t_Indx', 'cat36_t_Indx', 'cat37_t_Indx',\n",
    "               'cat38_t_Indx', 'cat39_t_Indx']\n",
    "\n",
    "cat_vecs = ['n1v', 'n2v', 'n3v', 'n4v', 'n5v', 'n6v', 'n7v',\n",
    "           'n8v', 'n9v', 'n10v', 'n11v', 'n12v', 'n13v',\n",
    "           'cat14v', 'cat15v', 'cat16v', 'cat17v',\n",
    "           'cat18v', 'cat19v', 'cat20v', 'cat21v', 'cat22v',\n",
    "           'cat23v', 'cat24v', 'cat25v', 'cat26v', 'cat27v',\n",
    "           'cat28v', 'cat29v', 'cat30v', 'cat31v', 'cat32v',\n",
    "           'cat33v', 'cat34v', 'cat35v', 'cat36v', 'cat37v',\n",
    "           'cat38v', 'cat39v']\n",
    "\n",
    "#convert categorical variables to numerical categories\n",
    "indexers = [StringIndexer(inputCol= c, \n",
    "                         outputCol=\"{0}_Indx\".format(c), \n",
    "                         handleInvalid=\"keep\") \n",
    "           for c in cat_cols]\n",
    "#Create a list representation of the numerical categorical representation\n",
    "encoder = OneHotEncoderEstimator(inputCols = [indexer.getOutputCol() for indexer in indexers], \n",
    "                                  outputCols = cat_vecs, \n",
    "                                  dropLast = True)\n",
    "#adding a assembler step which creates a sparse representation of an indexed column\n",
    "assembler = VectorAssembler(inputCols=encoder.getOutputCols(), \n",
    "                           outputCol = 'features')\n",
    "#creating the full transform pipeline  \n",
    "start = time.time()\n",
    "print(f'Running pipeline to create sparse vectors.. ')                            \n",
    "pipeline = Pipeline(stages = indexers + [encoder] + [assembler] )\n",
    "\n",
    "model = pipeline.fit(df_pq)\n",
    "\n",
    "transformed = model.transform(df_pq)\n",
    "\n",
    "drop_cols = cat_str_indx + cat_vecs\n",
    "final_df = transformed.drop(*drop_cols).cache()\n",
    "\n",
    "time_taken = time.time() - start\n",
    "print(f\"... completed job in {time_taken} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing file to parquet\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Write output file to parquet\n",
    "print('writing file to parquet')\n",
    "final_df.write.parquet(OUTPUT_FILE, compression='snappy', mode='overwrite')\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
