{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of decision tree algorithms (as it precursor to random forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree algorithm for classification is in the classification tree family of algorithms.\n",
    "\n",
    "A decision tree is a machine learning model based upon binary trees (trees with at most a left and right child). A decision tree learns the relationship between observations in a training set, represented as feature vectors $x$ and target values $y$, by examining and condensing training data into a binary tree of interior nodes and leaf nodes.\n",
    "\n",
    "A decision tree carves up the feature space into groups of observations that share similar target values and each leaf represents one of these groups. For classification, it means that most or all targets are of a single class.\n",
    "\n",
    "We predict that each feature instance belongs to the most commonly occurring class of feature observations in the region to which is belongs. Following the ISL section 8.1 reading, we are interested in the class proportions among the training observations falling into that feature region. Binary splits in growing the classification tree are made by using the classification error rate. The classification error rate is the fraction of feature value instances in the feature region that do not belong to the most common feature value:\n",
    "\n",
    "$\\begin{equation}\n",
    "E = 1-max_k(\\hat{p}_{mk})\\end{equation}$ where $\\hat{p}_{mk}$ represents the proportion of feature value observations in the $m$th feature region that are from the $k$th feature value class.\n",
    "\n",
    "However, since the classification error is not sensitive to the distribution of data points across binary splits (for tree-growing), and hence, the purity of a split, the entropy measure is preferred to measure the quality of a particular split. The entropy measure is given by:\n",
    "\n",
    "$D = - \\sum_{k=1}^{K}\\hat{p}_{mk}log\\hat{p}_{mk}$\n",
    "\n",
    "where 0 $\\le$ $\\hat{p}_{mk}$ $\\le$ 1 and 0$\\le$ $-\\hat{p}_{mk}log\\hat{p}_{mk}$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are an ensemble algorithm in which a number of decision trees are trained and then used for prediction in classification or regression. The decision trees are created on samples of the training data using a process called bootstrap aggregation or \"bagging\". Bagging works by sampling with replacement $B$ samples of size $n$ from a dataset $X$ giving datasets $X_i$ for $i \\in \\{1, ..., |B|\\}$. Each dataset $X_i$ is used to train its own decision tree, resulting in a total of $|B|$ decision trees. During inference, each tree in the ensemble receives a copy of the input and produces its own prediction. In the case of classification, the class with the most results is the final prediction. For regression, the predictions are averaged to produce a single prediction.\n",
    "\n",
    "Random forests improve over bagging by taking random samples of $m$ predictor features chosen as a subset of candidates for splitting from the full set of $p$ predictors. The split is allowed to use only one of the predictor features and a new sample of $m$ predictors is taken at each split. Typically, $m$ $\\approx$ $\\sqrt{p}$ such that the number of predictors considered at each split is equal to the square root of the total number of predictors (nominally, not even a majority of the entire set of predictors). This random sampling approach effectively reduces the variance attributed to a tree resulting from single strong predictor by considering only a subset of predictors each sample. The average prediction of the resulting trees is less variable and more reliable. That is, random forests effectively de-correlates the trees. The difference between bagging and random forests is the choice predictor subset size $m$, where $m$ $=$ $p$ implies bagging and $m$ $\\approx$ $\\sqrt{p}$ implies random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set autoscaling policy to run on dataproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create autoscaling policy:\n",
    "# gcloud dataproc autoscaling-policies import autoscaling-policy --source=autoscaling.yml\n",
    "#\n",
    "# to run:\n",
    "# python dataproc/submit_job_to_cluster.py \\\n",
    "# --project_id=${PROJECT_ID} \\\n",
    "# --zone=${ZONE} \\\n",
    "# --cluster_name=${CLUSTER_NAME} \\\n",
    "# --gcs_bucket=${BUCKET} \\\n",
    "# --key_file=${KEY} \\\n",
    "# --create_new_cluster \\\n",
    "# --pyspark_file=train.py \\\n",
    "# --instance_type=n1-highmem-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load libraries and set Spark configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.shuffle.service.enabled', 'true')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.port', '41099')\n",
      "('spark.app.name', 'random_forest')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.id', 'local-1576352004449')\n",
      "('spark.dynamicAllocation.enabled', 'true')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.driver.host', 'docker.w261')\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import (DecisionTreeClassifier,\n",
    "                                       RandomForestClassifier)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import (OneHotEncoderEstimator, StringIndexer,\n",
    "                                VectorAssembler, VectorIndexer)\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, column, udf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from sklearn import neighbors\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "app_name = \"random_forest\"\n",
    "\n",
    "# needed for autoscaling in dataproc\n",
    "SparkContext.setSystemProperty('spark.dynamicAllocation.enabled', 'true')\n",
    "SparkContext.setSystemProperty('spark.shuffle.service.enabled', 'true')\n",
    "\n",
    "#initiate the spark session\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "spark\n",
    "#print all the session options and settings\n",
    "for object in sc.getConf().getAll():\n",
    "    print(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion on data pre-processing prior to algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark provides a `RandomForestClassifier` algorithm in the `pyspark.ml.classification` package and we chose to use this rather than write one from scratch. In order to use this classifier, we must perform some preprocessing on the input data to get it into the format the algorithm expects.\n",
    "\n",
    "### Preprocesing - Random Forest\n",
    "\n",
    "1. One hot encode categorical features but only emit features for categorical values who have more than 10,000 instances\n",
    "2. Combine these one hot encoded features with the numerical values and vectorize them to a single dataframe column\n",
    "3. Convert the target variable from a boolean value to a string value\n",
    "\n",
    "For the first step, we chose to filter out categorical values who have less than 10,000 instances as there were several columns that had over 1 million distinct values (in the full dataset), many of which only had a handful of instances. If we were to include these in the final dataset, it would increase the dimensionality of our data by many magnitudes while leading to a high variance and failing to generalize.\n",
    "\n",
    "Many of the included algorithms in the `pyspark.ml` package require the data to be in a vectorized format, so this step was required. Given a list of columns, the `pyspark.ml.feature.VectorAssembler` class generates a single vector column of the concatenated features with the elements ordered by the input columns. For example given this dataframe:\n",
    "\n",
    "|`x1`|`x2`|`x3`|\n",
    "|---|---|---|\n",
    "|`5`|`3`|`-10`|\n",
    "\n",
    "The `VectorAssembler` would output this value to a new column: `[5,3,-10]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the target variable was changed from a boolean type to a string type which is required by the `pyspark.ml.feature.StringIndexer` class. The `StringIndexer` indexes each class and is required by the classification algorithms in PySpark.\n",
    "\n",
    "Since random forests do not require feature engineering and are able to handle `NA` values, no other preprocessing was done. This data was saved to parquet for later use when we run the training of our algorithm. This code can be found in `random_forest/transform.py` and was executed as a PySpark job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Training - Random Forest \n",
    " \n",
    "The data used for training was read in from the parquet files generated during the preprocessing step above but was first split into a train and test dataset with a 70/30 split. The training dataset was used for training and the test dataset was used for testing the performance of the trained model. This code for this section is located in `random_forest/train.py` and was also executed as a PySpark job.\n",
    "\n",
    "On our first several trainings, we noticed the loss was fairly high and we discovered the class imbalance in our dataset was the likely culprit since random forests are sensitive to class imbalances in the training data. Given an imbalance, they will bias towards the class(es) that have more examples. To address this, we performed an oversampling of the underrepresented class in the training dataset. In this case the \"not clicked\" class ($y = 0$) comprised around 75% of the dataset, while the \"clicked\" class ($y = 1$) comprised the remaining 25%. Our oversampling strategy then became to duplicate the \"clicked\" examples twice giving a more balanced dataset. This approach lowered our loss over 35% on the test dataset. Note that this oversampling was only performed on the training dataset and not the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset:\n",
      "train count =  32293\n",
      "train class = 0 24194\n",
      "train class = 1 8099\n",
      "test count =  13755\n",
      "----------------------------------------------------------------------------------------------------\n",
      "oversample dataset:\n",
      "train count =  48491\n",
      "train class = 0 24194\n",
      "train class = 1 24297\n",
      "Random forest classifier with 40 trees, max depth of 5 and max bins of 32\n",
      "Log loss:  0.7616105161396188\n",
      "+---+----------+---------------------------------------+----------------------------------------+------------------+\n",
      "|y  |prediction|rawPrediction                          |probability                             |log_loss          |\n",
      "+---+----------+---------------------------------------+----------------------------------------+------------------+\n",
      "|0  |0.0       |[25.212588834658035,14.787411165341963]|[0.6303147208664509,0.3696852791335491] |0.9951032322430656|\n",
      "|0  |0.0       |[24.443691892172758,15.55630810782724] |[0.6110922973043189,0.388907702695681]  |0.9444132316507237|\n",
      "|0  |0.0       |[24.790702984371617,15.209297015628389]|[0.6197675746092903,0.38023242539070967]|0.9669725674757117|\n",
      "|0  |0.0       |[24.038694035960376,15.961305964039626]|[0.6009673508990094,0.3990326491009907] |0.9187120381191678|\n",
      "|0  |0.0       |[25.042162565070157,14.957837434929838]|[0.626054064126754,0.373945935873246]   |0.9836440485055484|\n",
      "|0  |0.0       |[26.41911887221042,13.580881127789583] |[0.6604779718052605,0.3395220281947396] |1.0802164498622397|\n",
      "|0  |0.0       |[23.831756565424058,16.168243434575945]|[0.5957939141356015,0.40420608586439866]|0.905830417558923 |\n",
      "|0  |0.0       |[25.9098141865203,14.090185813479694]  |[0.6477453546630076,0.3522546453369924] |1.0434009406764144|\n",
      "|0  |0.0       |[24.156240389207028,15.843759610792972]|[0.6039060097301757,0.3960939902698243] |0.9261037467242907|\n",
      "|0  |0.0       |[25.320692057782964,14.679307942217035]|[0.6330173014445741,0.36698269855542587]|1.0024405749395753|\n",
      "+---+----------+---------------------------------------+----------------------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---+----------+---------------------------------------+----------------------------------------+-------------------+\n",
      "|y  |prediction|rawPrediction                          |probability                             |log_loss           |\n",
      "+---+----------+---------------------------------------+----------------------------------------+-------------------+\n",
      "|1  |0.0       |[24.792732769932215,15.207267230067787]|[0.6198183192483053,0.3801816807516947] |0.4783288773561718 |\n",
      "|1  |0.0       |[24.609894230648724,15.390105769351273]|[0.6152473557662181,0.3847526442337818] |0.4857308875402511 |\n",
      "|1  |0.0       |[24.81967015742985,15.180329842570147] |[0.6204917539357463,0.3795082460642537] |0.47724296381216774|\n",
      "|1  |0.0       |[25.125805693181054,14.874194306818945]|[0.6281451423295263,0.3718548576704736] |0.46498402085855356|\n",
      "|1  |0.0       |[25.930947391891397,14.069052608108604]|[0.648273684797285,0.35172631520271513] |0.43344231871359   |\n",
      "|1  |1.0       |[19.634955493318852,20.365044506681148]|[0.4908738873329713,0.5091261126670287] |0.7115680327878895 |\n",
      "|1  |1.0       |[18.617491310264747,21.382508689735253]|[0.46543728275661866,0.5345627172433813]|0.7647779222623017 |\n",
      "|1  |1.0       |[18.073480382657166,21.92651961734283] |[0.45183700956642914,0.5481629904335708]|0.7944337624854686 |\n",
      "|1  |1.0       |[19.761181736377527,20.23881826362247] |[0.49402954340943817,0.5059704565905617]|0.7051599591098838 |\n",
      "|1  |0.0       |[21.93833373561843,18.061666264381564] |[0.5484583433904608,0.4515416566095392] |0.6006439485675893 |\n",
      "+---+----------+---------------------------------------+----------------------------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def oversample(df, target_col='y', feature_col='features', majority_class='0.0', minority_class='1.0', percentage_over=2):\n",
    "    minority = df[df[target_col] == minority_class].cache()\n",
    "\n",
    "    for i in range(percentage_over):\n",
    "        df = df.unionAll(minority)\n",
    "\n",
    "    return df\n",
    "\n",
    "def log_loss(actual, predicted, epsilon=1e-15):\n",
    "    acutal = int(actual)\n",
    "    predicted = predicted[0]  # class 0\n",
    "\n",
    "    # clip value between [epsion, 1-epsilon]\n",
    "    predicted = max(predicted, epsilon)\n",
    "    predicted = min(predicted, 1-epsilon)\n",
    "\n",
    "    if actual == 1:\n",
    "        return -math.log(predicted)\n",
    "    else:\n",
    "        return -math.log(1 - predicted)\n",
    "\n",
    "\n",
    "spark.udf.register('log_loss', log_loss, DoubleType())\n",
    "log_loss_udf = udf(log_loss, DoubleType())\n",
    "\n",
    "# read in processed parquet file into an RDD\n",
    "# df = spark.read.parquet(\n",
    "#     'gs://w261-f19-team1/ohe_10k_numeric').\\\n",
    "#     select(['label', 'features']).\\\n",
    "#     where(col('label').isNotNull())\n",
    "    \n",
    "#import one hot encoded file\n",
    "#INPUT_FILE = 'gs://261_projectdata/261project_data/df_ohe_10k.parquet'\n",
    "INPUT_FILE = 'data/df_ohe_300.parquet'\n",
    "\n",
    "# read in Spark dataframe\n",
    "df = spark.read.parquet(INPUT_FILE).select('y', 'features')\n",
    "\n",
    "# split test/train datasets\n",
    "train, test = df.randomSplit([0.7, 0.3], seed=0)\n",
    "\n",
    "train = train.cache()\n",
    "test = test.cache()\n",
    "\n",
    "print('original dataset:')\n",
    "print('train count = ', train.count())\n",
    "print('train class = 0', train.filter(train.y == '0.0').count())\n",
    "print('train class = 1', train.filter(train.y == '1.0').count())\n",
    "print('test count = ', test.count())\n",
    "\n",
    "print('-' * 100)\n",
    "\n",
    "# make 2 artificial examples of clicks per each example since we have a 25/75 split of click/non-click examples\n",
    "train = oversample(train, minority_class = '1.0', majority_class = '0.0', percentage_over = 2).cache()\n",
    "\n",
    "print('oversample dataset:')\n",
    "print('train count = ', train.count())\n",
    "print('train class = 0', train.filter(train.y == '0.0').count())\n",
    "print('train class = 1', train.filter(train.y == '1.0').count())\n",
    "\n",
    "# random forest training\n",
    "n_trees = 40\n",
    "max_depth = 5\n",
    "max_bins = 32\n",
    "\n",
    "print(f'Random forest classifier with {n_trees} trees, max depth of {max_depth} and max bins of {max_bins}')\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol='features',\n",
    "                            labelCol='y', numTrees=n_trees, maxDepth=max_depth, maxBins=max_bins)\n",
    "rfModel = rf.fit(train)\n",
    "rfPredictions = rfModel.transform(test)\n",
    "rfPredictions = rfPredictions.select('y',\n",
    "                                     'prediction',\n",
    "                                     'rawPrediction',\n",
    "                                     'probability',\n",
    "                                     log_loss_udf('y', 'probability').alias('log_loss')).\\\n",
    "    cache()\n",
    "\n",
    "print('Log loss: ', rfPredictions.groupBy().mean('log_loss').collect()[0]['avg(log_loss)'])\n",
    "\n",
    "rfPredictions.filter(rfPredictions.y == '0.0').show(10, truncate=False)\n",
    "rfPredictions.filter(rfPredictions.y == '1.0').show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show confusion matrix metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 39.01987201077804 %\n",
      "Recall = 64.058612109483 %\n"
     ]
    }
   ],
   "source": [
    "analyze = rfPredictions.select('y', 'prediction')\n",
    "analyze = analyze.withColumn(\"prediction\", analyze[\"prediction\"])\n",
    "analyze = analyze.withColumn(\"y\", analyze[\"y\"])\n",
    "rfPredictions.unpersist()\n",
    "#Create SQL queries table\n",
    "sqlContext.registerDataFrameAsTable(analyze, \"results\")\n",
    "#Get metrics for precision recall calculations\n",
    "TP = spark.sql(\"\"\"SELECT COUNT(prediction)\n",
    "                    FROM results\n",
    "                    WHERE y = 1 AND prediction = 1\n",
    "                    \"\"\").collect()[0][0]\n",
    "\n",
    "TN = spark.sql(\"\"\"SELECT COUNT(prediction)\n",
    "                    FROM results\n",
    "                    WHERE y = 0 AND prediction = 0\n",
    "                    \"\"\").collect()[0][0]\n",
    "\n",
    "FP = spark.sql(\"\"\"SELECT COUNT(prediction)\n",
    "                    FROM results\n",
    "                    WHERE y = 0 AND prediction = 1\n",
    "                    \"\"\").collect()[0][0]\n",
    "\n",
    "FN = spark.sql(\"\"\"SELECT COUNT(prediction)\n",
    "                    FROM results\n",
    "                    WHERE y = 1 AND prediction = 0\n",
    "                    \"\"\").collect()[0][0]\n",
    "\n",
    "\n",
    "print(f'Precision = {TP/(TP + FP) * 100} %')\n",
    "\n",
    "print(f'Recall = {TP/(TP + FN) * 100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model outputs and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1883.save.\n: java.io.IOException: Path data/oversample_rf_40_5_32/model already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:702)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:179)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-dee2e4403b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# rfModel.save(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     f'gs://w261-f19-team1/oversample_rf_{n_trees}_{max_depth}_{max_bins}/model')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrfModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'data/oversample_rf_{n_trees}_{max_depth}_{max_bins}/model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# rfPredictions.write.parquet(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/pyspark-2.4.4-py3.7.egg/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/pyspark-2.4.4-py3.7.egg/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/py4j-0.10.7-py3.7.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/pyspark-2.4.4-py3.7.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/py4j-0.10.7-py3.7.egg/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1883.save.\n: java.io.IOException: Path data/oversample_rf_40_5_32/model already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:702)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:179)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# Save model outputs and predictions\n",
    "\n",
    "# rfModel.save(\n",
    "#     f'gs://w261-f19-team1/oversample_rf_{n_trees}_{max_depth}_{max_bins}/model')\n",
    "rfModel.save(f'data/oversample_rf_{n_trees}_{max_depth}_{max_bins}/model')\n",
    "\n",
    "# rfPredictions.write.parquet(\n",
    "#     f'gs://w261-f19-team1/oversample_rf_{n_trees}_{max_depth}_{max_bins}/predictions')\n",
    "\n",
    "rfPredictions.write.parquet(f'data/oversample_rf_{n_trees}_{max_depth}_{max_bins}/predictions', compression='snappy', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
