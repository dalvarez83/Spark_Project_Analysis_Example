{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration on loading files into parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal\n",
    "\n",
    "- To load a sample of a raw text file and transform into parquet file\n",
    "- Examine metadata of parquet partitions\n",
    "- Load parquet into a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/anaconda/lib/python3.7/site-packages (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/anaconda/lib/python3.7/site-packages (from pyarrow) (1.17.2)\n",
      "Requirement already satisfied: six>=1.0.0 in /opt/anaconda/lib/python3.7/site-packages (from pyarrow) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General tools & operations libraries\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "# Mathematical operations and dataframes libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting and visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Parquet libraries\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# PySpark libraries\n",
    "from pyspark.sql import SQLContext\n",
    "#from pyspark.sql import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters and Spark configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign parameters\n",
    "!BUCKET=danielalvarez_w261projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input and outfiles to store parquet files\n",
    "INPUT_FILES = 'data/sample.txt'\n",
    "#INPUT_FILES = 'gs://danielalvarez_w261projects/finalproject/sample.txt'\n",
    "OUT_FILES = 'gs://danielalvarez_w261projects/finalproject/df.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"finalproject_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.rdd.compress', 'True')\n",
      "('spark.driver.port', '37043')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.name', 'finalproject_notebook')\n",
      "('spark.app.id', 'local-1575325563237')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.driver.host', 'docker.w261')\n"
     ]
    }
   ],
   "source": [
    "# Spark configuration Information\n",
    "for object in sc.getConf().getAll():\n",
    "    print(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://docker.w261:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>finalproject_notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f10b05a0fd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in text file and create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  183029  6305520 44503676 data/sample.txt\n"
     ]
    }
   ],
   "source": [
    "# check number of bytes, lines, and words\n",
    "!wc data/sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t2\t-1\t\t\t501\t0\t2\t0\t0\t1\t1\t\t\t68fd1e64\t4c2bc594\td032c263\tc18be181\t25c83c98\tfe6b92e5\t1e9876db\t0b153874\ta73ee510\tfa7d0797\t043725ae\tdfbb09fb\t7f0d7407\t8ceecbc8\t7ac43a46\t84898b2a\t07c540c4\tbc48b783\t\t\t0014c32a\t\t3a171ecb\t3b183c5c\t\t\n"
     ]
    }
   ],
   "source": [
    "# inspect first line\n",
    "!head -n 1 data/sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file in as a Spark dataframe and check the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the schema \n",
    "df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"data/sample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0\t2\t-1\t\t\t501\t0\t2\t0\t0\t1\t1\t\t\t68fd1e64\t4c2bc594\td032c263\tc18be181\t25c83c98\tfe6b92e5\t1e9876db\t0b153874\ta73ee510\tfa7d0797\t043725ae\tdfbb09fb\t7f0d7407\t8ceecbc8\t7ac43a46\t84898b2a\t07c540c4\tbc48b783\t\t\t0014c32a\t\t3a171ecb\t3b183c5c\t\t: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183028"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impose schema structure. \n",
    "\n",
    "The 13th variable is a numeric (`n13`), 14th variable is categorical (`cat14`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 13th variable is a numeric (`n13`), 14th variable is categorical (`cat14`)\n",
    "schema = StructType([\n",
    "    StructField('y', IntegerType()),\n",
    "    StructField('n1', IntegerType()),\n",
    "    StructField('n2', IntegerType()),\n",
    "    StructField('n3', IntegerType()),\n",
    "    StructField('n4', IntegerType()),\n",
    "    StructField('n5', LongType()),\n",
    "    StructField('n6', IntegerType()),\n",
    "    StructField('n7', IntegerType()),\n",
    "    StructField('n8', IntegerType()),\n",
    "    StructField('n9', IntegerType()),\n",
    "    StructField('n10', IntegerType()),\n",
    "    StructField('n11', IntegerType()),\n",
    "    StructField('n12', IntegerType()),\n",
    "    StructField('n13', IntegerType()), \n",
    "    StructField('cat14', StringType()),\n",
    "    StructField('cat15', StringType()),\n",
    "    StructField('cat16', StringType()),\n",
    "    StructField('cat17', StringType()),\n",
    "    StructField('cat18', StringType()),\n",
    "    StructField('cat19', StringType()),\n",
    "    StructField('cat20', StringType()),\n",
    "    StructField('cat21', StringType()),\n",
    "    StructField('cat22', StringType()),\n",
    "    StructField('cat23', StringType()),\n",
    "    StructField('cat24', StringType()),\n",
    "    StructField('cat25', StringType()),\n",
    "    StructField('cat26', StringType()),\n",
    "    StructField('cat27', StringType()),\n",
    "    StructField('cat28', StringType()),\n",
    "    StructField('cat29', StringType()),\n",
    "    StructField('cat30', StringType()),\n",
    "    StructField('cat31', StringType()),\n",
    "    StructField('cat32', StringType()),\n",
    "    StructField('cat33', StringType()),\n",
    "    StructField('cat34', StringType()),\n",
    "    StructField('cat35', StringType()),\n",
    "    StructField('cat36', StringType()),\n",
    "    StructField('cat37', StringType()),\n",
    "    StructField('cat38', StringType()),\n",
    "    StructField('cat39', StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataframe..\n",
      "... completed job in 0.09658217430114746 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print('Creating dataframe..')\n",
    "df = spark.read.load(\"data/sample.txt\", format='csv', sep='\\t', header='false', schema=schema)\n",
    "#df = spark.read.load(INPUT_FILES, format='csv', sep='\\t', header='false', schema=schema)\n",
    "print(f\"... completed job in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the first 5 rows of selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+--------+--------+\n",
      "|  y|  n1| n12| n13|   cat14|   cat39|\n",
      "+---+----+----+----+--------+--------+\n",
      "|  0|   2|null|null|68fd1e64|    null|\n",
      "|  0|   3|   0|   1|68a25dc5|da9fe092|\n",
      "|  0|   0|null|   2|68fd1e64|    null|\n",
      "|  0|  64|   0|null|05db9164|    null|\n",
      "|  1|null|null|null|8cf07265|0a47000d|\n",
      "+---+----+----+----+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.select('y','n1','n12','n13','cat14','cat39').show(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183029"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write dataframe to parquet file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataframe to parquet format..\n",
      "... completed job in 4.0193445682525635 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print('Writing dataframe to parquet format..')\n",
    "\n",
    "df.write.parquet('data/df.parquet', compression='snappy', mode='overwrite')\n",
    "#df.write.parquet(OUT_FILES, compression='snappy', mode='overwrite')\n",
    "\n",
    "print(f\"... completed job in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pq = spark.read.load('data/df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183029\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# count the number of rows\n",
    "print(df_pq.count())\n",
    "\n",
    "# perform an assert to check number of rows matches before and after parquet conversion\n",
    "print(df_pq.count() == df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gsutil ls command with options -l (long listing) and -R (recursive listing) will list the entire bucket recursively and then produce a total count of all objects, both files and directories, at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil ls -lR gs://w261-f19-team1/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total, piping in the `tail` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil ls -lR gs://w261-f19-team1/train | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of files, bytes, and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil ls -lR gs://w261-f19-team1/train | wc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load parquet partition files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/df.parquet:\n",
      "total 12744\n",
      "-rw-r--r-- 1 root root       0 Dec  2 22:26 _SUCCESS\n",
      "-rw-r--r-- 1 root root 3529804 Dec  2 22:26 part-00000-b9c2b470-4ae1-423b-9aca-6976680fdb09-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3536883 Dec  2 22:26 part-00001-b9c2b470-4ae1-423b-9aca-6976680fdb09-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3544833 Dec  2 22:26 part-00002-b9c2b470-4ae1-423b-9aca-6976680fdb09-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 2430975 Dec  2 22:26 part-00003-b9c2b470-4ae1-423b-9aca-6976680fdb09-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lR data/df.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total of the last partition, piping in the `tail` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 2430975 Dec  2 22:26 part-00003-b9c2b470-4ae1-423b-9aca-6976680fdb09-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lR data/df.parquet | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of files, bytes, and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7      48     530\n"
     ]
    }
   ],
   "source": [
    "!ls -lR data/df.parquet | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the metadata of each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = ['data/df.parquet/part-00000-b9c2b470-4ae1-423b-9aca-6976680fdb09-c000.snappy.parquet', \n",
    "             'data/df.parquet/part-00001-b9c2b470-4ae1-423b-9aca-6976680fdb09-c000.snappy.parquet',\n",
    "             'data/df.parquet/part-00002-b9c2b470-4ae1-423b-9aca-6976680fdb09-c000.snappy.parquet',\n",
    "             'data/df.parquet/part-00003-b9c2b470-4ae1-423b-9aca-6976680fdb09-c000.snappy.parquet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.FileMetaData object at 0x7f10b049d830>\n",
      "  created_by: parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)\n",
      "  num_columns: 40\n",
      "  num_rows: 50037\n",
      "  num_row_groups: 1\n",
      "  format_version: 1.0\n",
      "  serialized_size: 6608\n",
      "<pyarrow._parquet.FileMetaData object at 0x7f10b049d890>\n",
      "  created_by: parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)\n",
      "  num_columns: 40\n",
      "  num_rows: 50139\n",
      "  num_row_groups: 1\n",
      "  format_version: 1.0\n",
      "  serialized_size: 6608\n",
      "<pyarrow._parquet.FileMetaData object at 0x7f10b049d710>\n",
      "  created_by: parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)\n",
      "  num_columns: 40\n",
      "  num_rows: 50059\n",
      "  num_row_groups: 1\n",
      "  format_version: 1.0\n",
      "  serialized_size: 6608\n",
      "<pyarrow._parquet.FileMetaData object at 0x7f10b049d8f0>\n",
      "  created_by: parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)\n",
      "  num_columns: 40\n",
      "  num_rows: 32794\n",
      "  num_row_groups: 1\n",
      "  format_version: 1.0\n",
      "  serialized_size: 6593\n"
     ]
    }
   ],
   "source": [
    "for file in partitions:\n",
    "    pqfile = pq.ParquetFile(file)\n",
    "    #table = pq.read_table('gs://w261-f19-team1/train/part-00003-7fa47152-b757-4ddd-ab8e-14738b4015f0-c000.snappy.parquet')\n",
    "    print(pqfile.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wc: data/df.parquet/part-00000-6dadf449-f385-4950-8167-6024f329f107-c000.snappy.parquet: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!wc data/df.parquet/part-00000-6dadf449-f385-4950-8167-6024f329f107-c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read parquet file as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_df = pd.read_parquet('data/df.parquet/part-00000-b9c2b470-4ae1-423b-9aca-6976680fdb09-c000.snappy.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>n3</th>\n",
       "      <th>n4</th>\n",
       "      <th>n5</th>\n",
       "      <th>n6</th>\n",
       "      <th>n7</th>\n",
       "      <th>n8</th>\n",
       "      <th>n9</th>\n",
       "      <th>...</th>\n",
       "      <th>cat30</th>\n",
       "      <th>cat31</th>\n",
       "      <th>cat32</th>\n",
       "      <th>cat33</th>\n",
       "      <th>cat34</th>\n",
       "      <th>cat35</th>\n",
       "      <th>cat36</th>\n",
       "      <th>cat37</th>\n",
       "      <th>cat38</th>\n",
       "      <th>cat39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>501.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>07c540c4</td>\n",
       "      <td>bc48b783</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0014c32a</td>\n",
       "      <td>None</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>3b183c5c</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>07c540c4</td>\n",
       "      <td>f54016b9</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>b1252a9d</td>\n",
       "      <td>5b4aa781</td>\n",
       "      <td>None</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>1793a828</td>\n",
       "      <td>e8b83407</td>\n",
       "      <td>da9fe092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15864.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27c07bd6</td>\n",
       "      <td>38748bc3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5a8fe828</td>\n",
       "      <td>None</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>f96a556f</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>e8623312</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>e106ec2a</td>\n",
       "      <td>None</td>\n",
       "      <td>423fab69</td>\n",
       "      <td>b889075b</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8020.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>7119e567</td>\n",
       "      <td>1d04f4a4</td>\n",
       "      <td>b1252a9d</td>\n",
       "      <td>d5f54153</td>\n",
       "      <td>None</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>a9d771cd</td>\n",
       "      <td>c9f3bea7</td>\n",
       "      <td>0a47000d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   y    n1  n2   n3   n4       n5     n6    n7    n8     n9  ...     cat30  \\\n",
       "0  0   2.0  -1  NaN  NaN    501.0    0.0   2.0   0.0    0.0  ...  07c540c4   \n",
       "1  0   3.0  -1  2.0  1.0     79.0    1.0   3.0   1.0    1.0  ...  07c540c4   \n",
       "2  0   0.0   0  3.0  2.0  15864.0  283.0   1.0   2.0  155.0  ...  27c07bd6   \n",
       "3  0  64.0   8  NaN  NaN    177.0    0.0  65.0  20.0   18.0  ...  e5ba7672   \n",
       "4  1   NaN  -1  NaN  NaN   8020.0   26.0   6.0   0.0   80.0  ...  e5ba7672   \n",
       "\n",
       "      cat31     cat32     cat33     cat34 cat35     cat36     cat37     cat38  \\\n",
       "0  bc48b783      None      None  0014c32a  None  3a171ecb  3b183c5c      None   \n",
       "1  f54016b9  21ddcdc9  b1252a9d  5b4aa781  None  32c7478e  1793a828  e8b83407   \n",
       "2  38748bc3      None      None  5a8fe828  None  32c7478e  f96a556f      None   \n",
       "3  e8623312      None      None  e106ec2a  None  423fab69  b889075b      None   \n",
       "4  7119e567  1d04f4a4  b1252a9d  d5f54153  None  32c7478e  a9d771cd  c9f3bea7   \n",
       "\n",
       "      cat39  \n",
       "0      None  \n",
       "1  da9fe092  \n",
       "2      None  \n",
       "3      None  \n",
       "4  0a47000d  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50037 entries, 0 to 50036\n",
      "Data columns (total 40 columns):\n",
      "y        50037 non-null int32\n",
      "n1       27891 non-null float64\n",
      "n2       50037 non-null int32\n",
      "n3       39260 non-null float64\n",
      "n4       39178 non-null float64\n",
      "n5       48694 non-null float64\n",
      "n6       38527 non-null float64\n",
      "n7       47884 non-null float64\n",
      "n8       50012 non-null float64\n",
      "n9       47884 non-null float64\n",
      "n10      27891 non-null float64\n",
      "n11      47884 non-null float64\n",
      "n12      11489 non-null float64\n",
      "n13      39178 non-null float64\n",
      "cat14    50037 non-null object\n",
      "cat15    50037 non-null object\n",
      "cat16    48314 non-null object\n",
      "cat17    48314 non-null object\n",
      "cat18    50037 non-null object\n",
      "cat19    43530 non-null object\n",
      "cat20    50037 non-null object\n",
      "cat21    50037 non-null object\n",
      "cat22    50037 non-null object\n",
      "cat23    50037 non-null object\n",
      "cat24    50037 non-null object\n",
      "cat25    48314 non-null object\n",
      "cat26    50037 non-null object\n",
      "cat27    50037 non-null object\n",
      "cat28    50037 non-null object\n",
      "cat29    48314 non-null object\n",
      "cat30    50037 non-null object\n",
      "cat31    50037 non-null object\n",
      "cat32    28442 non-null object\n",
      "cat33    28442 non-null object\n",
      "cat34    48314 non-null object\n",
      "cat35    11694 non-null object\n",
      "cat36    50037 non-null object\n",
      "cat37    48314 non-null object\n",
      "cat38    28442 non-null object\n",
      "cat39    28442 non-null object\n",
      "dtypes: float64(12), int32(2), object(26)\n",
      "memory usage: 14.9+ MB\n"
     ]
    }
   ],
   "source": [
    "part1_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
