{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will perform Logistic Regression. Logistic regression is appropriate since we are predicting a categorical target outcome given a feature set, $X$. \n",
    "\n",
    "Following ISL, section 4.3, we can express the logistic function for the log-odds ratio or logit for the multiple predictor case as:\n",
    "\n",
    "$\\begin{equation}log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_{0} + \\beta_{1} X_{1} +\\ldots + \\beta_{p} X_{p}\\end{equation}$\n",
    "\n",
    "where $X=(X_{1},\\ldots,X_{p})$ are $p$ predictors.\n",
    "\n",
    "Therefore, the logit regression model is linear in $X$. The logistic function can be written as the logit function, which expresses the underlying sigmoid function:\n",
    "\n",
    "$\\begin{equation}p(X)=\\frac{exp^{\\beta_{0} + \\beta_{1}X_{1} + \\ldots +\\beta_{p}X_{p}}}{1+ exp^{\\beta_{0} + \\beta_{1}X_{1} + \\ldots +\\beta_{p}X_{p}}}\\end{equation}$\n",
    "\n",
    "or, alternatively, as \n",
    "\n",
    "$\\begin{equation}logistic(\\boldsymbol{w}^T\\cdot\\mathbf{X}) = logit^{-1}(\\boldsymbol{w}^T\\cdot\\mathbf{X}) = \\frac{1}{1+exp^{-\\boldsymbol{w}^T\\cdot\\mathbf{X}'_i}}\\end{equation}$\n",
    "\n",
    "where  $\\boldsymbol{w}$  denotes the model parameters and $b$ represents the bias.\n",
    "\n",
    "The Logistic Regression loss function can be described mathematically as:\n",
    "\n",
    "$\\begin{equation}\n",
    "f(\\boldsymbol{w}) = \\sum_{i=1}^{n}log(1+exp^{-y(\\boldsymbol{w}^T\\cdot\\mathbf{x}'_i + b)})\\end{equation}$\n",
    "\n",
    "\n",
    "Or, loss (L)<br>\n",
    "\n",
    "$\\begin{equation}L = \\frac{1}{2N} \\sum_{i}(y_{i} - a_{i})^2\\end{equation}$\n",
    "<br>\n",
    "\n",
    "Using chain rule we can reduce this to : <br>\n",
    "\n",
    "$\\begin{equation}\\frac{dL}{dW} = \\frac{1}{N} \\sum_{i}-(y_{i} - a_{i}).a_{i}(1-a_{i}).x_{i}\\end{equation}$\n",
    "\n",
    "\n",
    "#### Evaluation criteria\n",
    "\n",
    "Log loss is used to evaluate logistic regression and tree-based algorithms. It is the negative log-likelihood of the Bernoulli model. As mentioned in the reference paper, Aryafar, et al. (2017), log loss is often used as an indicator for model performance in online advertising. Minimizing log loss indicates that the conditional probability of clicking on an ad given a query by the user should converge to the expected click rate. Here the conditional probability can be written an $P(c|q,l)$, where $c$ is the binary response of clicking on an advertisement or not, $q$ is the indicator for a query issued by a user and $l$ is a given advertisement listing. A model with lower log loss is preferred. This is referred to in the reference paper Aryafar, et al. (2017).\n",
    "\n",
    "The following logarithmic loss function is used to measure the evaluate the model proposed:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\boldsymbol{logloss} = -\\frac{1}{L}\\sum_{i=1}^{L}y_{i}log p_{i}+(1-y_{i})log(1-p_{i})\\end{equation}$\n",
    "\n",
    "where $L$ is the number of instances, $y_{i} \\in {0,1}$ is the label of the $i^{th}$ instance, and $p_{i}$ is the probability of that the $i^{th}$ instance is clicked. \n",
    "\n",
    "As far as confusion matrix metrics, precision and recall are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the example guidance for implementation in MLLib in:\n",
    "\n",
    "https://github.com/MingChen0919/learning-apache-spark/blob/master/notebooks/06-machine-learning/classification/logistic-regression.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run, execute in bash shell\n",
    "# python submit_job_to_bigger_cluster.py --project_id=${PROJECT_ID} --zone=central1-a --cluster_name=w261finalproject --gcs_bucket=${BUCKET_NAME} --key_file=$HOME/MIDS/w261.json --create_new_cluster --pyspark_file=data_processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.driver.port', '36825')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.id', 'local-1576351808175')\n",
      "('spark.app.name', 'logistic_regression')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.driver.host', 'docker.w261')\n"
     ]
    }
   ],
   "source": [
    "# load and import libaries\n",
    "import time\n",
    "import math\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit, when, col, approx_count_distinct, mean, log, udf\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "\n",
    "app_name = \"logistic_regression\"\n",
    "\n",
    "#set spark session options\n",
    "conf=SparkConf()\n",
    "conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "#initiate the spark session\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#print all the session options and settings\n",
    "for object in sc.getConf().getAll():\n",
    "    print(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://docker.w261:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>logistic_regression</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcc86470510>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign global variables and input and output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the global variables\n",
    "MAX_ITER = 10       #number of model training epochs\n",
    "THRESHOLD = 0.45    #model training threshold for classification probability\n",
    "EPSILON = 1e-16     #variable to force bounded solution for log transforms\n",
    "\n",
    "#import one hot encoded file\n",
    "#INPUT_FILE = 'gs://261_projectdata/261project_data/df_ohe_10k.parquet'\n",
    "INPUT_FILE = 'data/df_ohe_300.parquet'\n",
    "\n",
    "# OUTPUT_FILE = 'gs://261_projectdata/results/ohe_10k_result.csv'\n",
    "OUTPUT_FILE = 'data/ohe_result.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in parquet file into a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total number of Rows = 46048\n"
     ]
    }
   ],
   "source": [
    "df_pq = spark.read.parquet(INPUT_FILE)\\\n",
    "                .select('y', 'features')\n",
    "\n",
    "print(f' Total number of Rows = {df_pq.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dropped NULL values from prediction variable. Total number of Rows = 46048\n",
      " Total number of Rows in the TEST set = 9282\n",
      "+------+--------------------+\n",
      "|labels|            features|\n",
      "+------+--------------------+\n",
      "|     0|(545,[0,11,29,52,...|\n",
      "|     0|(545,[0,11,29,52,...|\n",
      "|     0|(545,[0,11,29,52,...|\n",
      "|     0|(545,[0,11,29,52,...|\n",
      "|     0|(545,[0,11,29,52,...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      " Total number of Rows in the TRAIN set = 36766\n",
      "+--------------------+------+\n",
      "|            features|labels|\n",
      "+--------------------+------+\n",
      "|(545,[0,11,29,52,...|     0|\n",
      "|(545,[0,11,29,52,...|     0|\n",
      "|(545,[0,11,29,52,...|     0|\n",
      "|(545,[0,11,29,52,...|     0|\n",
      "|(545,[0,11,29,52,...|     0|\n",
      "|(545,[0,11,29,52,...|     0|\n",
      "|(545,[0,11,29,52,...|     0|\n",
      "|(545,[0,11,29,52,...|     0|\n",
      "|(545,[0,11,29,52,...|     0|\n",
      "|(545,[0,11,29,52,...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Positive class in the training data = 25.444704346407004 %\n",
      "Baseline Log loss for the training data = 0.5671681910174072\n",
      "Training model...\n",
      "Training completed ...in 1.6370890140533447\n"
     ]
    }
   ],
   "source": [
    "#remove records where the label is missing\n",
    "df_pq = df_pq.na.drop(subset = [\"y\"])\n",
    "\n",
    "print(f' Dropped NULL values from prediction variable. Total number of Rows = {df_pq.count()}')\n",
    "#create train-test split\n",
    "train_y_all, test_y_all = df_pq.randomSplit([0.8, 0.2])\n",
    "df_pq.unpersist()\n",
    "\n",
    "#change the name of the dependent variable column\n",
    "test = test_y_all.selectExpr(\"y as labels\", \"features\")\n",
    "print(f' Total number of Rows in the TEST set = {test.count()}')\n",
    "test.show(n=5)\n",
    "#force a binary classification 0 or 1 on the training data and save as column 'labels'\n",
    "train_y = train_y_all.withColumn('labels', when(col('y') > 0.0, lit(1)).otherwise(lit(0)))\n",
    "train_y_all.unpersist()\n",
    "#remove the old dependent variable\n",
    "train = train_y.drop('y')\n",
    "train_y.unpersist()\n",
    "\n",
    "print(f' Total number of Rows in the TRAIN set = {train.count()}')\n",
    "\n",
    "train.show(n=10)\n",
    "\n",
    "\n",
    "#Get the percentage of values in the positive class, to set the baseline log loss\n",
    "mean_prob = train.select(mean(col('labels'))).collect()[0]['avg(labels)']\n",
    "print(f'Positive class in the training data = {mean_prob * 100} %')\n",
    "\n",
    "#assign mean probability as the baseline probability estimate\n",
    "train_prob = train.withColumn('base_prob', lit(mean_prob))\n",
    "train.unpersist()\n",
    "\n",
    "# calculate baseline logloss  \n",
    "train_ll = train_prob.withColumn('logloss', when(col('labels') > 0.0, - log(col('base_prob') + EPSILON))\\\n",
    "                                       .otherwise( - log(1.0 - col('base_prob') + EPSILON)))\n",
    "train_prob.unpersist()\n",
    "#get the mean of the logloss\n",
    "mean_ll = train_ll.select(mean(col('logloss'))).collect()[0]['avg(logloss)']\n",
    "print(f'Baseline Log loss for the training data = {mean_ll}')\n",
    "\n",
    "#specify the model and hyperparameters\n",
    "lr = LogisticRegression(featuresCol = 'features',\n",
    "                        labelCol='labels', \n",
    "                        maxIter = MAX_ITER, \n",
    "                        standardization = False,\n",
    "                        elasticNetParam = 0.0, \n",
    "                        threshold = THRESHOLD\n",
    "                        )\n",
    "\n",
    "start = time.time()\n",
    "print('Training model...')\n",
    "lr_model = lr.fit(train_ll)\n",
    "time_taken = time.time() - start\n",
    "print(f'Training completed ...in {time_taken}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define user defined functions for sigmoid function and extracting vector; then extract coefficients and confusion matrix metrics (precision and recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Log loss for the training data = 0.5671681910174072\n",
      "MODEL Log loss for the training data = 0.5012941120993041\n",
      "+--------------------+------+----------+-------------------+--------------------+-------------------+\n",
      "|            features|labels|prediction|          calc_prob|         probability|            logloss|\n",
      "+--------------------+------+----------+-------------------+--------------------+-------------------+\n",
      "|(545,[0,11,29,52,...|     0|       0.0| 0.2454877025064577|[0.75451229749354...| 0.2816837020421912|\n",
      "|(545,[0,11,29,52,...|     0|       0.0|0.11046406477582439|[0.88953593522417...|0.11705537333963632|\n",
      "|(545,[0,11,29,52,...|     0|       0.0| 0.0666469095631895|[0.93335309043681...|0.06897170338584309|\n",
      "|(545,[0,11,29,52,...|     0|       0.0|0.32543339638649177|[0.67456660361350...|0.39368486304473055|\n",
      "|(545,[0,11,29,52,...|     0|       0.0| 0.3880836650158173|[0.61191633498418...| 0.4911597133573236|\n",
      "|(545,[0,11,29,52,...|     0|       0.0| 0.2028108255449076|[0.79718917445509...|0.22666327019494079|\n",
      "|(545,[0,11,29,52,...|     0|       0.0| 0.1160036397229418|[0.88399636027705...|0.12330233368661431|\n",
      "|(545,[0,11,29,52,...|     0|       0.0| 0.0720052591040666|[0.92799474089593...|0.07472921334999748|\n",
      "|(545,[0,11,29,52,...|     0|       0.0|0.06938534822184184|[0.93061465177815...|0.07190999525306598|\n",
      "|(545,[0,11,29,52,...|     0|       0.0|0.16309127869394482|[0.83690872130605...|0.17804026903207268|\n",
      "+--------------------+------+----------+-------------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Precision = 56.57276995305164 %\n",
      "Recall = 30.62261753494282 %\n",
      "+-----------+------------+\n",
      "|    feature|       coeff|\n",
      "+-----------+------------+\n",
      "|n1v_1000001|-0.030013168|\n",
      "|      n1v_0|-0.045568857|\n",
      "|      n1v_1| -0.07480369|\n",
      "|      n1v_2|-0.015737953|\n",
      "|      n1v_3|  0.21428978|\n",
      "+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(rawPrediction):\n",
    "    #apply affine transformation\n",
    "    return 1 / (1 + math.exp(- rawPrediction))\n",
    "\n",
    "def extract_from_vector(vec, i):\n",
    "    \"\"\" Input: VectorUDT data type\n",
    "        Output: float type at index (i)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return float(vec[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "#register UDFs    \n",
    "get_probability = udf(sigmoid, DoubleType())\n",
    "get_val = udf(extract_from_vector, DoubleType())\n",
    "\n",
    "#Get predictions on test data\n",
    "result = lr_model.transform(test)\n",
    "\n",
    "#Apply affine transformation to the linear regression output\n",
    "result2 = result.withColumn('calc_prob', get_probability(get_val(\"rawPrediction\", lit(1))))\n",
    "# calculate log loss\n",
    "result3 = result2.withColumn('logloss', when(col('labels') > 0.0, - log(col('calc_prob') + EPSILON))\\\n",
    "                                       .otherwise( - log(1.0 - col('calc_prob') + EPSILON)))\n",
    "\n",
    "\n",
    "test_mean_ll = result3.select(mean(col('logloss'))).collect()[0]['avg(logloss)']\n",
    "print(f'Baseline Log loss for the training data = {mean_ll}')\n",
    "print(f'MODEL Log loss for the training data = {test_mean_ll}')\n",
    "\n",
    "result3.select(['features', 'labels' ,'prediction', 'calc_prob', 'probability', 'logloss']).show(n=10, truncate=True)\n",
    "\n",
    "analyze = result3.select('labels', 'prediction')\n",
    "analyze = analyze.withColumn(\"prediction\", analyze[\"prediction\"].cast(IntegerType()))\n",
    "analyze = analyze.withColumn(\"labels\", analyze[\"labels\"].cast(IntegerType()))\n",
    "result3.unpersist()\n",
    "#Create SQL queries table\n",
    "sqlContext.registerDataFrameAsTable(analyze, \"results\")\n",
    "#Get metrics for precision recall calculations\n",
    "TP = spark.sql(\"\"\"SELECT COUNT(prediction)\n",
    "                    FROM results\n",
    "                    WHERE labels = 1 AND prediction = 1\n",
    "                    \"\"\").collect()[0][0]\n",
    "\n",
    "TN = spark.sql(\"\"\"SELECT COUNT(prediction)\n",
    "                    FROM results\n",
    "                    WHERE labels = 0 AND prediction = 0\n",
    "                    \"\"\").collect()[0][0]\n",
    "\n",
    "FP = spark.sql(\"\"\"SELECT COUNT(prediction)\n",
    "                    FROM results\n",
    "                    WHERE labels = 0 AND prediction = 1\n",
    "                    \"\"\").collect()[0][0]\n",
    "\n",
    "FN = spark.sql(\"\"\"SELECT COUNT(prediction)\n",
    "                    FROM results\n",
    "                    WHERE labels = 1 AND prediction = 0\n",
    "                    \"\"\").collect()[0][0]\n",
    "\n",
    "\n",
    "print(f'Precision = {TP/(TP + FP) * 100} %')\n",
    "\n",
    "print(f'Recall = {TP/(TP + FN) * 100} %')\n",
    "\n",
    "features = [x[\"name\"] for x in sorted(train_ll.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"binary\"], \n",
    "                                      key=lambda x: x[\"idx\"]\n",
    "                                     )]\n",
    "\n",
    "#Extract feature names and indices from table metadata\n",
    "schema = StructType([StructField(\"feature\", StringType()),\n",
    "                    StructField(\"coeff\", FloatType())\n",
    "                    ])\n",
    "\n",
    "# Create dataframe of coefficients\n",
    "result_df = spark.createDataFrame(zip(features, lr_model.coefficients.tolist()), schema=schema)\n",
    "print(result_df.show(n=5))\n",
    "\n",
    "#save to CSV file\n",
    "result_df.coalesce(1).write.csv(OUTPUT_FILE, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Hashing Function - Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the global variables\n",
    "NUM_FEATURES = 2**18  #parameter for setting the number of mapped feature space\n",
    "MAX_ITER = 10       #number of model training epochs\n",
    "THRESHOLD = 0.45    #model training threshold for classification probability\n",
    "EPSILON = 1e-16     #variable to force bounded solution for log trransforms\n",
    "\n",
    "\n",
    "# INPUT_FILE = 'gs://261_projectdata/261project_data/df.parquet'\n",
    "INPUT_FILE = 'data/df.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign column features and parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cols = ['n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7',\n",
    "           'n8', 'n9', 'n10', 'n11', 'n12',\n",
    "           'n13','cat14', 'cat15', 'cat16', 'cat17',\n",
    "           'cat18', 'cat19', 'cat20', 'cat21', 'cat22',\n",
    "           'cat23', 'cat24', 'cat25', 'cat26', 'cat27',\n",
    "           'cat28', 'cat29', 'cat30', 'cat31', 'cat32',\n",
    "           'cat33', 'cat34', 'cat35', 'cat36', 'cat37',\n",
    "           'cat38', 'cat39']\n",
    "\n",
    "df_pq = spark.read.parquet(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Hashed feature with 262144 number of vectors ...\n",
      "Hashed feature vector completed ...in 0.08662009239196777\n",
      "+---+--------------------+\n",
      "|  y|            features|\n",
      "+---+--------------------+\n",
      "|  0|(262144,[3723,576...|\n",
      "|  0|(262144,[3723,398...|\n",
      "|  0|(262144,[3723,654...|\n",
      "|  0|(262144,[3723,141...|\n",
      "|  1|(262144,[5768,129...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Creating train-test split..\n",
      "Creating test set..\n",
      "+------+--------------------+\n",
      "|labels|            features|\n",
      "+------+--------------------+\n",
      "|     0|(262144,[26,5929,...|\n",
      "|     0|(262144,[41,3723,...|\n",
      "|     0|(262144,[67,3723,...|\n",
      "|     0|(262144,[76,3723,...|\n",
      "|     0|(262144,[89,3723,...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Creating train set..\n",
      "+--------------------+------+\n",
      "|            features|labels|\n",
      "+--------------------+------+\n",
      "|(262144,[9,15026,...|     0|\n",
      "|(262144,[13,3723,...|     0|\n",
      "|(262144,[32,15944...|     0|\n",
      "|(262144,[35,3723,...|     0|\n",
      "|(262144,[35,3723,...|     0|\n",
      "|(262144,[38,3723,...|     0|\n",
      "|(262144,[38,3723,...|     0|\n",
      "|(262144,[41,3723,...|     0|\n",
      "|(262144,[44,3723,...|     0|\n",
      "|(262144,[44,3723,...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Positive class in the training data = 25.35573229454093 %\n",
      "Baseline Log loss for the training data = 0.5662096236906365\n",
      "Training model...\n",
      "Training completed ...in 4.8809814453125\n",
      "Baseline Log loss for the training data = 0.5662096236906365\n",
      "MODEL Log loss for the training data = 1.4418082938447523\n",
      "Precision = 38.56920684292379 %\n",
      "Recall = 31.75416133162612 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Creating Hashed feature with {NUM_FEATURES} number of vectors ...')\n",
    "start = time.time()\n",
    "\n",
    "hasher = FeatureHasher(inputCols=my_cols, outputCol=\"features\", numFeatures = NUM_FEATURES)\n",
    "\n",
    "hash_transformed = hasher.transform(df_pq).select('y', 'features')\n",
    "print(f'Hashed feature vector completed ...in {time.time() - start}')\n",
    "\n",
    "df_pq.unpersist()\n",
    "\n",
    "hash_transformed.show(n=5, truncate=True)\n",
    "#create train-test split\n",
    "print('Creating train-test split..')\n",
    "train_y_all, test_y_all = hash_transformed.randomSplit([0.8, 0.2], seed=10)\n",
    "hash_transformed.unpersist()\n",
    "#change the name of the dependent variable column\n",
    "print('Creating test set..')\n",
    "test = test_y_all.selectExpr(\"y as labels\", \"features\")\n",
    "test.show(n=5)\n",
    "#force a binary classification 0 or 1 on the training data and save as column 'labels'\n",
    "print('Creating train set..')\n",
    "train_y = train_y_all.withColumn('labels', when(col('y') > 0.0, lit(1)).otherwise(lit(0)))\n",
    "train_y_all.unpersist()\n",
    "\n",
    "train = train_y.drop('y')\n",
    "train_y.unpersist()\n",
    "\n",
    "train.show(n=10)\n",
    "\n",
    "\n",
    "#Get the percentage of values in the positive class, to set the baseline log loss\n",
    "mean_prob = train.select(mean(col('labels'))).collect()[0]['avg(labels)']\n",
    "print(f'Positive class in the training data = {mean_prob * 100} %')\n",
    "\n",
    "#assign mean probability as the baseline probability estimate\n",
    "train_prob = train.withColumn('base_prob', lit(mean_prob))\n",
    "train.unpersist()\n",
    "\n",
    "# calculate baseilne logloss  \n",
    "train_ll = train_prob.withColumn('logloss', when(col('labels') == 1.0, - log(col('base_prob') + EPSILON))\\\n",
    "                                       .otherwise( - log(1.0 - col('base_prob') + EPSILON)))\n",
    "train_prob.unpersist()\n",
    "\n",
    "mean_ll = train_ll.select(mean(col('logloss'))).collect()[0]['avg(logloss)']\n",
    "print(f'Baseline Log loss for the training data = {mean_ll}')\n",
    "#specify the model and hyperparameters\n",
    "lr = LogisticRegression(featuresCol = 'features',\n",
    "                        labelCol='labels', \n",
    "                        maxIter = MAX_ITER, \n",
    "                        standardization = False,\n",
    "                        elasticNetParam = 0.0, \n",
    "                        threshold = THRESHOLD\n",
    "                        )\n",
    "\n",
    "start = time.time()\n",
    "print('Training model...')\n",
    "lr_model = lr.fit(train_ll)\n",
    "time_taken = time.time() - start\n",
    "print(f'Training completed ...in {time_taken}')\n",
    "\n",
    "def sigmoid(rawPrediction):\n",
    "    #apply affine transformation\n",
    "    return 1 / (1 + math.exp(- rawPrediction))\n",
    "\n",
    "def extract_from_vector(vec, i):\n",
    "    \"\"\" Input: VectorUDT data type\n",
    "        Output: float type at index (i)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return float(vec[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "#register UDFs      \n",
    "get_probability = udf(sigmoid, DoubleType())\n",
    "get_val = udf(extract_from_vector, DoubleType())\n",
    "\n",
    "#Get predictions on test data\n",
    "result = lr_model.transform(test)\n",
    "\n",
    "#Apply affine transformation to the linear regression output\n",
    "result2 = result.withColumn('calc_prob', get_probability(get_val(\"rawPrediction\", lit(1))))\n",
    "# calculate log loss\n",
    "result3 = result2.withColumn('logloss', when(col('labels') > 0.0, - log(col('calc_prob') + EPSILON))\\\n",
    "                                       .otherwise( - log(1.0 - col('calc_prob') + EPSILON)))\n",
    "\n",
    "\n",
    "test_mean_ll = result3.select(mean(col('logloss'))).collect()[0]['avg(logloss)']\n",
    "print(f'Baseline Log loss for the training data = {mean_ll}')\n",
    "print(f'MODEL Log loss for the training data = {test_mean_ll}')\n",
    "\n",
    "analyze = result3.select('labels', 'prediction')\n",
    "analyze = analyze.withColumn(\"prediction\", analyze[\"prediction\"].cast(IntegerType()))\n",
    "analyze = analyze.withColumn(\"labels\", analyze[\"labels\"].cast(IntegerType()))\n",
    "\n",
    "#Create SQL queries table\n",
    "sqlContext.registerDataFrameAsTable(analyze, \"results\")\n",
    "#Get metrics for precision recall calculations\n",
    "TP = spark.sql(\"\"\"SELECT COUNT(prediction)\n",
    "                    FROM results\n",
    "                    WHERE labels = 1 AND prediction = 1\n",
    "                    \"\"\").collect()[0][0]\n",
    "\n",
    "TN = spark.sql(\"\"\"SELECT COUNT(prediction)\n",
    "                    FROM results\n",
    "                    WHERE labels = 0 AND prediction = 0\n",
    "                    \"\"\").collect()[0][0]\n",
    "\n",
    "FP = spark.sql(\"\"\"SELECT COUNT(prediction)\n",
    "                    FROM results\n",
    "                    WHERE labels = 0 AND prediction = 1\n",
    "                    \"\"\").collect()[0][0]\n",
    "\n",
    "FN = spark.sql(\"\"\"SELECT COUNT(prediction)\n",
    "                    FROM results\n",
    "                    WHERE labels = 1 AND prediction = 0\n",
    "                    \"\"\").collect()[0][0]\n",
    "\n",
    "P = spark.sql(\"\"\"SELECT COUNT(labels)\n",
    "                    FROM results\n",
    "                    WHERE labels = 1\n",
    "                    \"\"\").collect()[0][0]\n",
    "\n",
    "N = spark.sql(\"\"\"SELECT COUNT(labels)\n",
    "                    FROM results\n",
    "                    WHERE labels = 0\n",
    "                    \"\"\").collect()[0][0]\n",
    "\n",
    "print(f'Precision = {TP/(TP + FP) * 100} %')\n",
    "\n",
    "print(f'Recall = {TP/(TP + FN) * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
