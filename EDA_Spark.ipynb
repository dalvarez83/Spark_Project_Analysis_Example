{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (using Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description:\n",
    "\n",
    "This notebook shows the EDA performed for the w261 final project. This EDA started with taking a small random sample of the raw data and performing exploratory analysis with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/ee/fd2d696eff911f76ed14feeb51e6db6783dd04abd9b8e14be4cbf48d6088/pyarrow-0.15.1-cp37-cp37m-manylinux2010_x86_64.whl (59.2MB)\n",
      "\u001b[K     |████████████████████████████████| 59.2MB 58.7MB/s eta 0:00:01    |███████████████                 | 27.6MB 3.4MB/s eta 0:00:10\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /opt/anaconda/lib/python3.7/site-packages (from pyarrow) (1.17.2)\n",
      "Requirement already satisfied: six>=1.0.0 in /opt/anaconda/lib/python3.7/site-packages (from pyarrow) (1.12.0)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-0.15.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General tools & operations libraries\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "# Mathematical operations and dataframes libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting and visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Parquet libraries\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# PySpark libraries\n",
    "from pyspark.sql import SQLContext\n",
    "#from pyspark.sql import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters and Spark configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign parameters\n",
    "!BUCKET=danielalvarez_w261projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"finalproject_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.app.id', 'local-1575325734557')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.port', '35303')\n",
      "('spark.app.name', 'finalproject_notebook')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.driver.host', 'docker.w261')\n"
     ]
    }
   ],
   "source": [
    "# Spark configuration Information\n",
    "for object in sc.getConf().getAll():\n",
    "    print(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://docker.w261:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>finalproject_notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f573aa5dcd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine 2-3 relevant EDA tasks that will help you make decisions about how you implement the algorithm to be scalable. Discuss any challenges that you anticipate based on the EDA you perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset represents 0.4% of the raw `train.txt` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!cat train.txt | awk 'BEGIN {srand()} !/^$/ { if (rand() <= .001) print $0}' > data/sample.txt\n",
    "#!gzip -cd data/dac.tar.gz | awk 'BEGIN {srand()} !/^$/ { if (rand() <= .001) print $0}' > data/sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impose schema structure. \n",
    "\n",
    "The 13th variable is a numeric (`n13`), 14th variable is categorical (`cat14`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the 13th variable is a numeric (`n13`), 14th variable is categorical (`cat14`)\n",
    "schema = StructType([\n",
    "    StructField('y', IntegerType()),\n",
    "    StructField('n1', IntegerType()),\n",
    "    StructField('n2', IntegerType()),\n",
    "    StructField('n3', IntegerType()),\n",
    "    StructField('n4', IntegerType()),\n",
    "    StructField('n5', LongType()),\n",
    "    StructField('n6', IntegerType()),\n",
    "    StructField('n7', IntegerType()),\n",
    "    StructField('n8', IntegerType()),\n",
    "    StructField('n9', IntegerType()),\n",
    "    StructField('n10', IntegerType()),\n",
    "    StructField('n11', IntegerType()),\n",
    "    StructField('n12', IntegerType()),\n",
    "    StructField('n13', IntegerType()), \n",
    "    StructField('cat14', StringType()),\n",
    "    StructField('cat15', StringType()),\n",
    "    StructField('cat16', StringType()),\n",
    "    StructField('cat17', StringType()),\n",
    "    StructField('cat18', StringType()),\n",
    "    StructField('cat19', StringType()),\n",
    "    StructField('cat20', StringType()),\n",
    "    StructField('cat21', StringType()),\n",
    "    StructField('cat22', StringType()),\n",
    "    StructField('cat23', StringType()),\n",
    "    StructField('cat24', StringType()),\n",
    "    StructField('cat25', StringType()),\n",
    "    StructField('cat26', StringType()),\n",
    "    StructField('cat27', StringType()),\n",
    "    StructField('cat28', StringType()),\n",
    "    StructField('cat29', StringType()),\n",
    "    StructField('cat30', StringType()),\n",
    "    StructField('cat31', StringType()),\n",
    "    StructField('cat32', StringType()),\n",
    "    StructField('cat33', StringType()),\n",
    "    StructField('cat34', StringType()),\n",
    "    StructField('cat35', StringType()),\n",
    "    StructField('cat36', StringType()),\n",
    "    StructField('cat37', StringType()),\n",
    "    StructField('cat38', StringType()),\n",
    "    StructField('cat39', StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataframe..\n",
      "... completed job in 1.8787925243377686 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print('Creating dataframe..')\n",
    "df = spark.read.load(\"data/sample.txt\", format='csv', sep='\\t', header='false', schema=schema)\n",
    "print(f\"... completed job in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the first 5 rows of selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+--------+--------+\n",
      "|  y|  n1| n12| n13|   cat14|   cat39|\n",
      "+---+----+----+----+--------+--------+\n",
      "|  0|   2|null|null|68fd1e64|    null|\n",
      "|  0|   3|   0|   1|68a25dc5|da9fe092|\n",
      "|  0|   0|null|   2|68fd1e64|    null|\n",
      "|  0|  64|   0|null|05db9164|    null|\n",
      "|  1|null|null|null|8cf07265|0a47000d|\n",
      "+---+----+----+----+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.select('y','n1','n12','n13','cat14','cat39').show(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183029"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(y=0, n1=2, n2=-1, n3=None, n4=None, n5=501, n6=0, n7=2, n8=0, n9=0, n10=1, n11=1, n12=None, n13=None, cat14='68fd1e64', cat15='4c2bc594', cat16='d032c263', cat17='c18be181', cat18='25c83c98', cat19='fe6b92e5', cat20='1e9876db', cat21='0b153874', cat22='a73ee510', cat23='fa7d0797', cat24='043725ae', cat25='dfbb09fb', cat26='7f0d7407', cat27='8ceecbc8', cat28='7ac43a46', cat29='84898b2a', cat30='07c540c4', cat31='bc48b783', cat32=None, cat33=None, cat34='0014c32a', cat35=None, cat36='3a171ecb', cat37='3b183c5c', cat38=None, cat39=None),\n",
       " Row(y=0, n1=3, n2=-1, n3=2, n4=1, n5=79, n6=1, n7=3, n8=1, n9=1, n10=1, n11=1, n12=0, n13=1, cat14='68a25dc5', cat15='80e26c9b', cat16='3b40a9aa', cat17='37dff460', cat18='25c83c98', cat19=None, cat20='815e3303', cat21='0b153874', cat22='a73ee510', cat23='b9b1972c', cat24='2cfc1696', cat25='ba5646a2', cat26='9bbdb8bd', cat27='07d13a8f', cat28='f3635baf', cat29='cb880c3a', cat30='07c540c4', cat31='f54016b9', cat32='21ddcdc9', cat33='b1252a9d', cat34='5b4aa781', cat35=None, cat36='32c7478e', cat37='1793a828', cat38='e8b83407', cat39='da9fe092'),\n",
       " Row(y=0, n1=0, n2=0, n3=3, n4=2, n5=15864, n6=283, n7=1, n8=2, n9=155, n10=0, n11=1, n12=None, n13=2, cat14='68fd1e64', cat15='08d6d899', cat16='04f0a0fb', cat17='1bfb072a', cat18='25c83c98', cat19='13718bbd', cat20='744efce0', cat21='5b392875', cat22='a73ee510', cat23='c3e69838', cat24='7a3651f5', cat25='cd31013f', cat26='95bc260c', cat27='8ceecbc8', cat28='54747c3e', cat29='8295d26f', cat30='27c07bd6', cat31='38748bc3', cat32=None, cat33=None, cat34='5a8fe828', cat35=None, cat36='32c7478e', cat37='f96a556f', cat38=None, cat39=None),\n",
       " Row(y=0, n1=64, n2=8, n3=None, n4=None, n5=177, n6=0, n7=65, n8=20, n9=18, n10=1, n11=2, n12=0, n13=None, cat14='05db9164', cat15='38a947a1', cat16='c4b406e2', cat17='bf536db3', cat18='25c83c98', cat19='7e0ccccf', cat20='9fc5abd8', cat21='0b153874', cat22='a73ee510', cat23='4d1812fe', cat24='8b92652b', cat25='e8a073ad', cat26='c5bc951e', cat27='b28479f6', cat28='1db94996', cat29='489b1305', cat30='e5ba7672', cat31='e8623312', cat32=None, cat33=None, cat34='e106ec2a', cat35=None, cat36='423fab69', cat37='b889075b', cat38=None, cat39=None),\n",
       " Row(y=1, n1=None, n2=-1, n3=None, n4=None, n5=8020, n6=26, n7=6, n8=0, n9=80, n10=None, n11=2, n12=None, n13=None, cat14='8cf07265', cat15='b80912da', cat16='e51edcbe', cat17='90f40919', cat18='25c83c98', cat19='6f6d9be8', cat20='59434e5e', cat21='1f89b562', cat22='a73ee510', cat23='3b08e48b', cat24='a04db730', cat25='b57ec450', cat26='c66b30f8', cat27='07d13a8f', cat28='569913cf', cat29='11fe787a', cat30='e5ba7672', cat31='7119e567', cat32='1d04f4a4', cat33='b1252a9d', cat34='d5f54153', cat35=None, cat36='32c7478e', cat37='a9d771cd', cat38='c9f3bea7', cat39='0a47000d')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write dataframe to parquet file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataframe to parquet format..\n",
      "... completed job in 4.1327736377716064 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print('Writing dataframe to parquet format..')\n",
    "\n",
    "df.write.parquet('data/df.parquet', compression='snappy', mode='overwrite')\n",
    "#df.write.parquet(OUT_FILES, compression='snappy', mode='overwrite')\n",
    "\n",
    "print(f\"... completed job in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pq = spark.read.load('data/df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183029\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# count the number of rows\n",
    "print(df_pq.count())\n",
    "\n",
    "# perform an assert to check number of rows matches before and after parquet conversion\n",
    "print(df_pq.count() == df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(y=0, n1=None, n2=1, n3=2, n4=2, n5=23480, n6=361, n7=2, n8=2, n9=157, n10=None, n11=1, n12=None, n13=2, cat14='68fd1e64', cat15='7008ef6d', cat16='08e19f66', cat17='ded6a29a', cat18='25c83c98', cat19='fbad5c96', cat20='1c63b114', cat21='0b153874', cat22='a73ee510', cat23='f6f942d1', cat24='67841877', cat25='e7e7e539', cat26='781f4d92', cat27='07d13a8f', cat28='03259d67', cat29='ccf3df7a', cat30='e5ba7672', cat31='d1c83925', cat32=None, cat33=None, cat34='3336022d', cat35='ad3062eb', cat36='3a171ecb', cat37='b2f178a3', cat38=None, cat39=None),\n",
       " Row(y=0, n1=0, n2=57, n3=16, n4=27, n5=9105, n6=277, n7=26, n8=15, n9=1070, n10=0, n11=5, n12=0, n13=27, cat14='05db9164', cat15='7b99bba3', cat16='3c548aa7', cat17='96cc0f03', cat18='25c83c98', cat19=None, cat20='1c4d06eb', cat21='1f89b562', cat22='a73ee510', cat23='36c6971d', cat24='3168dd4c', cat25='8e5c8813', cat26='060905ec', cat27='b28479f6', cat28='b5de5085', cat29='bc8707ae', cat30='27c07bd6', cat31='42235923', cat32='21ddcdc9', cat33='5840adea', cat34='01d9669f', cat35=None, cat36='32c7478e', cat37='b1ec9c5d', cat38='c9f3bea7', cat39='69148c9d'),\n",
       " Row(y=0, n1=1, n2=1, n3=None, n4=3, n5=1524, n6=25, n7=2, n8=1, n9=3, n10=0, n11=1, n12=None, n13=3, cat14='05db9164', cat15='7008ef6d', cat16=None, cat17=None, cat18='43b19349', cat19='7e0ccccf', cat20='72121ef3', cat21='0b153874', cat22='a73ee510', cat23='df2138cf', cat24='1b723d3c', cat25=None, cat26='2e072c0d', cat27='07d13a8f', cat28='03259d67', cat29=None, cat30='3486227d', cat31='d1c83925', cat32=None, cat33=None, cat34=None, cat35=None, cat36='3a171ecb', cat37=None, cat38=None, cat39=None),\n",
       " Row(y=1, n1=0, n2=0, n3=118, n4=7, n5=1542, n6=57, n7=1, n8=7, n9=7, n10=0, n11=1, n12=None, n13=7, cat14='05db9164', cat15='1cfdf714', cat16='cf6e068e', cat17='aa76e087', cat18='384874ce', cat19='fe6b92e5', cat20='fa19d92a', cat21='5b392875', cat22='a73ee510', cat23='31990058', cat24='1054ae5c', cat25='93c5771e', cat26='d7ce3abd', cat27='b28479f6', cat28='d345b1a0', cat29='c9cea2f5', cat30='d4bb7bd8', cat31='e88ffc9d', cat32='21ddcdc9', cat33='a458ea53', cat34='a1cc3401', cat35=None, cat36='bcdee96c', cat37='3fdb382b', cat38='cb079c2d', cat39='49d68486'),\n",
       " Row(y=0, n1=0, n2=1, n3=19, n4=3, n5=172, n6=198, n7=6, n8=40, n9=109, n10=0, n11=2, n12=0, n13=3, cat14='05db9164', cat15='207b2d81', cat16='905a7b53', cat17='0e5acd1d', cat18='25c83c98', cat19='fbad5c96', cat20='dda1fed2', cat21='25239412', cat22='a73ee510', cat23='441dd290', cat24='7f8ffe57', cat25='bac9dcdb', cat26='46f42a63', cat27='cfef1c29', cat28='18c35602', cat29='301d75b3', cat30='e5ba7672', cat31='157482f0', cat32='21ddcdc9', cat33='a458ea53', cat34='278a31a1', cat35=None, cat36='3a171ecb', cat37='5bceb83a', cat38='001f3601', cat39='8221c0ef')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine first 5 rows\n",
    "df_pq.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load files to GCP bucket and convert to RDDs for Spark analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train.txt` and `test.txt` files were downloaded to an external hard drive and subsequently loaded into a GCP bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This command streams the main data set from dropbox directly to your GCP bucket - this may take a little time (RUN THIS CELL AS IS)\n",
    "#!curl -L \"https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz\" | gsutil cp - gs://{BUCKET}/finalproject/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not run in the Docker container. This command puts a local file on GCP\n",
    "#!gsutil cp 'train.txt' gs://{BUCKET}/finalproject/train.txt\n",
    "#!gsutil cp 'train.txt' gs://danielalvarez_w261projects/finalproject/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not run in the Docker container. This command puts a local file on GCP\n",
    "#!gsutil cp 'test.txt' gs://{BUCKET}/finalproject/test.txt\n",
    "#!gsutil cp 'test.txt' gs://danielalvarez_w261projects/finalproject/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data into Spark RDDs for convenience of use later (RUN THIS CELL AS IS)\n",
    "# trainRDD = sc.textFile(f'gs://danielalvarez_w261projects/finalproject/train.txt')\n",
    "# testRDD = sc.textFile(f'gs://danielalvarez_w261projects/finalproject/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the class\n",
    "# print(type(trainRDD))\n",
    "# print(type(testRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of rows and shape of the files\n",
    "# !cat trainRDD | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to RDDs to DataFrames\n",
    "#DF = trainRDD.map(lambda x: (x.split('\\t')[0], ast.literal_eval(x.split('\\t')[1]))).toDF()\n",
    "# from pyspark.sql.types import Row\n",
    "\n",
    "# #here you are going to create a function\n",
    "# def f(x):\n",
    "#     d = {}\n",
    "#     for i in range(len(x)):\n",
    "#         d[str(i)] = x[i]\n",
    "#     return d\n",
    "\n",
    "# #Now populate that\n",
    "# df = trainRDD.map(lambda x: Row(**f(x))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
