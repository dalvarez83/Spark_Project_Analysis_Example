{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal of analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go step-by-step from EDA to feature engineering process. Use techniques of one-hot encoding, assemblers and vectorizers to create feature engineered dataset to be used in algorithm implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/anaconda/lib/python3.7/site-packages (0.15.1)\n",
      "Requirement already satisfied: six>=1.0.0 in /opt/anaconda/lib/python3.7/site-packages (from pyarrow) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/anaconda/lib/python3.7/site-packages (from pyarrow) (1.17.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General tools & operations libraries\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "# Mathematical operations and dataframes libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting and visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Parquet libraries\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# PySpark libraries\n",
    "from pyspark.sql import SQLContext\n",
    "#from pyspark.sql import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "from pyspark.sql.functions import lit, when, col, approx_count_distinct, udf, log, exp, abs, mean\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, OneHotEncoder, OneHotEncoderEstimator, VectorAssembler, FeatureHasher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters and Spark configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign parameters\n",
    "!BUCKET=danielalvarez_w261projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"finalproject_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.app.id', 'local-1575999037643')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.name', 'finalproject_notebook')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.driver.host', 'docker.w261')\n",
      "('spark.driver.port', '37599')\n"
     ]
    }
   ],
   "source": [
    "# Spark configuration Information\n",
    "for object in sc.getConf().getAll():\n",
    "    print(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://docker.w261:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>finalproject_notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f927f8cad10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine 2-3 relevant EDA tasks that will help you make decisions about how you implement the algorithm to be scalable. Discuss any challenges that you anticipate based on the EDA you perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset represents 0.1% of the raw `train.txt` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!cat train.txt | awk 'BEGIN {srand()} !/^$/ { if (rand() <= .001) print $0}' > data/sample.txt\n",
    "#!gzip -cd data/dac.tar.gz | awk 'BEGIN {srand()} !/^$/ { if (rand() <= .001) print $0}' > data/sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impose schema structure. \n",
    "\n",
    "The 13th variable is a numeric (`n13`), 14th variable is categorical (`cat14`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the 13th variable is a numeric (`n13`), 14th variable is categorical (`cat14`)\n",
    "schema = StructType([\n",
    "    StructField('y', IntegerType()), StructField('n1', IntegerType()),\n",
    "    StructField('n2', IntegerType()), StructField('n3', IntegerType()),\n",
    "    StructField('n4', IntegerType()), StructField('n5', LongType()),\n",
    "    StructField('n6', IntegerType()), StructField('n7', IntegerType()),\n",
    "    StructField('n8', IntegerType()), StructField('n9', IntegerType()),\n",
    "    StructField('n10', IntegerType()), StructField('n11', IntegerType()),\n",
    "    StructField('n12', IntegerType()), StructField('n13', IntegerType()), \n",
    "    StructField('cat14', StringType()), StructField('cat15', StringType()),\n",
    "    StructField('cat16', StringType()), StructField('cat17', StringType()),\n",
    "    StructField('cat18', StringType()), StructField('cat19', StringType()),\n",
    "    StructField('cat20', StringType()), StructField('cat21', StringType()),\n",
    "    StructField('cat22', StringType()), StructField('cat23', StringType()),\n",
    "    StructField('cat24', StringType()), StructField('cat25', StringType()),\n",
    "    StructField('cat26', StringType()), StructField('cat27', StringType()),\n",
    "    StructField('cat28', StringType()), StructField('cat29', StringType()),\n",
    "    StructField('cat30', StringType()), StructField('cat31', StringType()),\n",
    "    StructField('cat32', StringType()), StructField('cat33', StringType()),\n",
    "    StructField('cat34', StringType()), StructField('cat35', StringType()),\n",
    "    StructField('cat36', StringType()), StructField('cat37', StringType()),\n",
    "    StructField('cat38', StringType()), StructField('cat39', StringType()) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataframe..\n",
      "... completed job in 1.8539607524871826 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print('Creating dataframe..')\n",
    "df = spark.read.load(\"data/sample.txt\", format='csv', sep='\\t', header='false', schema=schema)\n",
    "print(f\"... completed job in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the first 5 rows of selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+---+--------+--------+\n",
      "|  y|  n1| n12|n13|   cat14|   cat39|\n",
      "+---+----+----+---+--------+--------+\n",
      "|  0|   1|null|  2|05db9164|    null|\n",
      "|  0|null|null|  1|05db9164|553d46e8|\n",
      "|  0|   1|null|  2|be589b51|64f08cc6|\n",
      "|  0|   0|null|  9|05db9164|    null|\n",
      "|  0|   0|   0| 24|05db9164|    null|\n",
      "+---+----+----+---+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.select('y','n1','n12','n13','cat14','cat39').show(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46048"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(y=0, n1=1, n2=2, n3=2, n4=2, n5=292, n6=2, n7=1, n8=2, n9=2, n10=1, n11=1, n12=None, n13=2, cat14='05db9164', cat15='0a519c5c', cat16='b00d1501', cat17='d16679b9', cat18='25c83c98', cat19='7e0ccccf', cat20='1683df22', cat21='0b153874', cat22='a73ee510', cat23='3b08e48b', cat24='89073265', cat25='e0d76380', cat26='8b266858', cat27='b28479f6', cat28='b760dcb7', cat29='1203a270', cat30='d4bb7bd8', cat31='2efa89c6', cat32=None, cat33=None, cat34='73d06dde', cat35=None, cat36='3a171ecb', cat37='aee52b6f', cat38=None, cat39=None),\n",
       " Row(y=0, n1=None, n2=-1, n3=86, n4=1, n5=6147, n6=875, n7=3, n8=1, n9=96, n10=None, n11=1, n12=None, n13=1, cat14='05db9164', cat15='4e8d18ed', cat16='a415cb70', cat17='554f4454', cat18='25c83c98', cat19='13718bbd', cat20='2045039f', cat21='0b153874', cat22='a73ee510', cat23='3b08e48b', cat24='5f8383cb', cat25='ef9491f3', cat26='86684160', cat27='1adce6ef', cat28='bbee52f9', cat29='90e3c135', cat30='e5ba7672', cat31='47e4d79e', cat32='9437f62f', cat33='a458ea53', cat34='d2bc432c', cat35=None, cat36='bcdee96c', cat37='b0a08858', cat38='c9f3bea7', cat39='553d46e8'),\n",
       " Row(y=0, n1=1, n2=1, n3=16, n4=2, n5=0, n6=2, n7=24, n8=2, n9=13, n10=1, n11=6, n12=None, n13=2, cat14='be589b51', cat15='1cfdf714', cat16='af1e24ee', cat17='d1de383b', cat18='25c83c98', cat19='fbad5c96', cat20='7195046d', cat21='0b153874', cat22='a73ee510', cat23='c2ebde4f', cat24='4d8549da', cat25='291a0d2a', cat26='51b97b8f', cat27='1adce6ef', cat28='f3002fbd', cat29='1b4d5a76', cat30='e5ba7672', cat31='e88ffc9d', cat32='efa3470f', cat33='b1252a9d', cat34='65229b6e', cat35=None, cat36='bcdee96c', cat37='3fdb382b', cat38='cb079c2d', cat39='64f08cc6'),\n",
       " Row(y=0, n1=0, n2=1, n3=3, n4=9, n5=3195, n6=57, n7=3, n8=35, n9=120, n10=0, n11=2, n12=None, n13=9, cat14='05db9164', cat15='08d6d899', cat16='6a8a1217', cat17='14bfebf4', cat18='25c83c98', cat19=None, cat20='eca465ad', cat21='0b153874', cat22='a73ee510', cat23='3b08e48b', cat24='ca465a9f', cat25='5b355b50', cat26='75d33e1a', cat27='64c94865', cat28='a8e4fe6e', cat29='0ded9094', cat30='e5ba7672', cat31='9dde83ca', cat32=None, cat33=None, cat34='831d5286', cat35=None, cat36='32c7478e', cat37='9e9a60e4', cat38=None, cat39=None),\n",
       " Row(y=0, n1=0, n2=21, n3=5, n4=6, n5=7460, n6=192, n7=2, n8=24, n9=153, n10=0, n11=2, n12=0, n13=24, cat14='05db9164', cat15='d833535f', cat16='b00d1501', cat17='d16679b9', cat18='25c83c98', cat19='7e0ccccf', cat20='5547e1f4', cat21='d3334ebc', cat22='a73ee510', cat23='5db9788f', cat24='087dfcfd', cat25='e0d76380', cat26='5317f239', cat27='07d13a8f', cat28='2e7bc615', cat29='1203a270', cat30='07c540c4', cat31='7b49e3d2', cat32=None, cat33=None, cat34='73d06dde', cat35=None, cat36='3a171ecb', cat37='aee52b6f', cat38=None, cat39=None)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write dataframe to parquet file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataframe to parquet format..\n",
      "... completed job in 2.378223180770874 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print('Writing dataframe to parquet format..')\n",
    "\n",
    "df.write.parquet('data/df.parquet', compression='snappy', mode='overwrite')\n",
    "#df.write.parquet(OUT_FILES, compression='snappy', mode='overwrite')\n",
    "\n",
    "print(f\"... completed job in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pq = spark.read.load('data/df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46048\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# count the number of rows\n",
    "print(df_pq.count())\n",
    "\n",
    "# perform an assert to check number of rows matches before and after parquet conversion\n",
    "print(df_pq.count() == df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read a parquet file as a pandas dataframe\n",
    "\n",
    "For subsequent analysis on the algorithm explanation (Section 2) and EDA (Section 3) we will use the dataframe created from a parquet partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_sample = pd.read_parquet('data/df.parquet/part-00000-a472c6d5-727b-4d11-b306-3bbb786d2b5b-c000.snappy.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count distinct values for each of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vars = ['n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13',\n",
    "        'cat14', 'cat15', 'cat16', 'cat17', 'cat18', 'cat19', 'cat20', 'cat21', 'cat22', \n",
    "        'cat23', 'cat23', 'cat24', 'cat25', 'cat26', 'cat27', 'cat28', 'cat29', 'cat30',\n",
    "        'cat31', 'cat32', 'cat33', 'cat34', 'cat35', 'cat36', 'cat37', 'cat38', 'cat39']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in n1 column: 112\n",
      "Distinct values in n2 column: 1920\n",
      "Distinct values in n3 column: 442\n",
      "Distinct values in n4 column: 85\n",
      "Distinct values in n5 column: 15280\n",
      "Distinct values in n6 column: 1401\n",
      "Distinct values in n7 column: 492\n",
      "Distinct values in n8 column: 67\n",
      "Distinct values in n9 column: 1288\n",
      "Distinct values in n10 column: 10\n",
      "Distinct values in n11 column: 87\n",
      "Distinct values in n12 column: 61\n",
      "Distinct values in n13 column: 134\n",
      "Distinct values in cat14 column: 361\n",
      "Distinct values in cat15 column: 493\n",
      "Distinct values in cat16 column: 23415\n",
      "Distinct values in cat17 column: 14183\n",
      "Distinct values in cat18 column: 102\n",
      "Distinct values in cat19 column: 11\n",
      "Distinct values in cat20 column: 6375\n",
      "Distinct values in cat21 column: 180\n",
      "Distinct values in cat22 column: 3\n",
      "Distinct values in cat23 column: 8075\n",
      "Distinct values in cat23 column: 8075\n",
      "Distinct values in cat24 column: 3407\n",
      "Distinct values in cat25 column: 21722\n",
      "Distinct values in cat26 column: 2612\n",
      "Distinct values in cat27 column: 24\n",
      "Distinct values in cat28 column: 4036\n",
      "Distinct values in cat29 column: 18586\n",
      "Distinct values in cat30 column: 10\n",
      "Distinct values in cat31 column: 2075\n",
      "Distinct values in cat32 column: 986\n",
      "Distinct values in cat33 column: 4\n",
      "Distinct values in cat34 column: 20395\n",
      "Distinct values in cat35 column: 10\n",
      "Distinct values in cat36 column: 13\n",
      "Distinct values in cat37 column: 7428\n",
      "Distinct values in cat38 column: 51\n",
      "Distinct values in cat39 column: 5714\n"
     ]
    }
   ],
   "source": [
    "for v in vars:\n",
    "    distinct_var = df_pq.select(v).distinct().count()\n",
    "    print('Distinct values in {} column: {}'.format(str(v),distinct_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distinct count analysis shows that the features `n5` , `cat16`, `cat17`, `cat25`, `cat29` and `cat34`, each have a large number of unique values (over 10K unique values). These values will be under-represented occurrences in the dataset requiring feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the number of distinct values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate distinct values in the dataset - 164840\n"
     ]
    }
   ],
   "source": [
    "dist_vals = df_pq.agg(*[approx_count_distinct(c).alias(c) for c in df_pq.columns])\\\n",
    "                .rdd.flatMap(lambda x: x)\\\n",
    "                .sum()\n",
    "\n",
    "print(f'Approximate distinct values in the dataset - {dist_vals}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct categories in cat17 column\n",
      "14183\n"
     ]
    }
   ],
   "source": [
    "# show distinct values for cat17 \n",
    "print('distinct categories in cat17 column')\n",
    "dist_cat17 = df_pq.select(\"cat17\").distinct().count()\n",
    "print(dist_cat17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct categories in n1 column\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "# show distinct values for n1\n",
    "print('distinct categories in n1 column')\n",
    "dist_n1 = df_pq.select(\"n1\").distinct().count()\n",
    "print(dist_n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(y=0, n1=1, n2=1, n3=5, n4=3, n5=375, n6=39, n7=11, n8=37, n9=106, n10=1, n11=3, n12=None, n13=3, cat14='5a9ed9b0', cat15='89ddfee8', cat16='2655adf1', cat17='c6b0e462', cat18='25c83c98', cat19='3bf701e7', cat20='82ffe275', cat21='0b153874', cat22='a73ee510', cat23='39da7128', cat24='e4c6cf60', cat25='266ce9dd', cat26='c4c46dce', cat27='1adce6ef', cat28='c77661b0', cat29='1ac05730', cat30='e5ba7672', cat31='bf27d32b', cat32='3014a4b1', cat33='5840adea', cat34='3b57a504', cat35='ad3062eb', cat36='bcdee96c', cat37='d2e445d0', cat38='f0f449dd', cat39='96d4bf8f'),\n",
       " Row(y=0, n1=None, n2=5, n3=32, n4=2, n5=63925, n6=None, n7=0, n8=4, n9=2, n10=None, n11=0, n12=None, n13=2, cat14='05db9164', cat15='a07503cc', cat16='c8b9f273', cat17='13508380', cat18='4cf72387', cat19='fbad5c96', cat20='08a6c211', cat21='5b392875', cat22='7cc72ec2', cat23='00f2b452', cat24='41b3f655', cat25='5938b690', cat26='ce5114a2', cat27='07d13a8f', cat28='77660bba', cat29='5ba2964d', cat30='3486227d', cat31='912c7e21', cat32='21ddcdc9', cat33='b1252a9d', cat34='22295680', cat35=None, cat36='32c7478e', cat37='45ab94c8', cat38='445bbe3b', cat39='c84c4aec'),\n",
       " Row(y=0, n1=None, n2=9, n3=1, n4=7, n5=1700, n6=43, n7=3, n8=7, n9=53, n10=None, n11=2, n12=0, n13=7, cat14='5a9ed9b0', cat15='2c16a946', cat16='cb3387e7', cat17='64712dc5', cat18='25c83c98', cat19=None, cat20='dda1fed2', cat21='931c3bf7', cat22='a73ee510', cat23='2462946f', cat24='7f8ffe57', cat25='85598c3b', cat26='46f42a63', cat27='07d13a8f', cat28='e083c32b', cat29='28588e50', cat30='3486227d', cat31='96af95cc', cat32=None, cat33=None, cat34='14b35491', cat35=None, cat36='32c7478e', cat37='9117a34a', cat38=None, cat39=None),\n",
       " Row(y=0, n1=10, n2=63, n3=14, n4=22, n5=223, n6=44, n7=15, n8=45, n9=353, n10=1, n11=3, n12=2, n13=38, cat14='5a9ed9b0', cat15='ed7b1c58', cat16='03d42cd1', cat17='77aeb6cb', cat18='4cf72387', cat19='7e0ccccf', cat20='3f0b24b0', cat21='1f89b562', cat22='a73ee510', cat23='e0e3498f', cat24='a353f58d', cat25='cd15d6f4', cat26='b22a5f52', cat27='07d13a8f', cat28='cf4a3e0f', cat29='5f2f8c2d', cat30='8efede7f', cat31='0e393340', cat32=None, cat33=None, cat34='5680575f', cat35='c9d4222a', cat36='32c7478e', cat37='0b8baf69', cat38=None, cat39=None),\n",
       " Row(y=1, n1=0, n2=0, n3=1, n4=None, n5=1496, n6=22, n7=45, n8=19, n9=33, n10=0, n11=12, n12=None, n13=None, cat14='05db9164', cat15='89ddfee8', cat16='8aa48f66', cat17='8d83e4b3', cat18='0942e0a7', cat19='7e0ccccf', cat20='1c86e0eb', cat21='37e4aa92', cat22='a73ee510', cat23='4cd49990', cat24='755e4a50', cat25='bd4f78f6', cat26='5978055e', cat27='07d13a8f', cat28='4df3da6b', cat29='7e2e0293', cat30='e5ba7672', cat31='5bb2ec8e', cat32='21ddcdc9', cat33='a458ea53', cat34='edb7e787', cat35=None, cat36='32c7478e', cat37='3fdb382b', cat38='f0f449dd', cat39='49d68486')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine first 5 rows of parquet file\n",
    "df_pq.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply transformations to the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a criteria to filter our values of features with a count less than a determined threshold as discussed in the de Wit (2014) paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Category count threshold \n",
    "# Set threshold equal to 10,000\n",
    "THRESHOLD = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_col = 'y'\n",
    "#numeric_columns = [f'_c{i}' for i in range(1, 13)]\n",
    "#category_columns = [f'_c{i}' for i in range(13, 30)]\n",
    "\n",
    "# Column names which will be transformed\n",
    "category_columns = ['cat14', 'cat15', 'cat16', 'cat17', 'cat18', 'cat19', 'cat20', 'cat21', 'cat22',\n",
    "           'cat23', 'cat24', 'cat25', 'cat26', 'cat27', 'cat28', 'cat29', 'cat30', 'cat31', \n",
    "           'cat32', 'cat33', 'cat34', 'cat35', 'cat36', 'cat37', 'cat38', 'cat39']\n",
    "\n",
    "numeric_columns = ['n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7',\n",
    "           'n8', 'n9', 'n10', 'n11', 'n12', 'n13']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total number of Rows = 46048\n"
     ]
    }
   ],
   "source": [
    "# StringIndexer requires string not boolean type\n",
    "df_pq = df_pq.withColumn(target_col, col(target_col).cast('string'))\n",
    "\n",
    "print(f' Total number of Rows = {df_pq.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define transformation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a function to filter each feature to contain unique values each with a threshold number of counts (filter out low occurrence values). For features with over a threshold number of unique values, we decide to create a stash variable that comprises the least represented values for each original feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming Categorical column.. cat14\n",
      "... completed job in 3.81636643409729 seconds\n",
      "Transforming Categorical column.. cat15\n",
      "... completed job in 3.001173734664917 seconds\n",
      "Transforming Categorical column.. cat16\n",
      "... completed job in 2.8838021755218506 seconds\n",
      "Transforming Categorical column.. cat17\n",
      "... completed job in 2.8992531299591064 seconds\n",
      "Transforming Categorical column.. cat18\n",
      "... completed job in 2.7315545082092285 seconds\n",
      "Transforming Categorical column.. cat19\n",
      "... completed job in 2.7182397842407227 seconds\n",
      "Transforming Categorical column.. cat20\n",
      "... completed job in 2.8201398849487305 seconds\n",
      "Transforming Categorical column.. cat21\n",
      "... completed job in 2.7374377250671387 seconds\n",
      "Transforming Categorical column.. cat22\n",
      "... completed job in 2.7233762741088867 seconds\n",
      "Transforming Categorical column.. cat23\n",
      "... completed job in 3.1336135864257812 seconds\n",
      "Transforming Categorical column.. cat24\n",
      "... completed job in 2.792780637741089 seconds\n",
      "Transforming Categorical column.. cat25\n",
      "... completed job in 2.7276358604431152 seconds\n",
      "Transforming Categorical column.. cat26\n",
      "... completed job in 2.7409439086914062 seconds\n",
      "Transforming Categorical column.. cat27\n",
      "... completed job in 2.7008237838745117 seconds\n",
      "Transforming Categorical column.. cat28\n",
      "... completed job in 2.6978583335876465 seconds\n",
      "Transforming Categorical column.. cat29\n",
      "... completed job in 2.7409472465515137 seconds\n",
      "Transforming Categorical column.. cat30\n",
      "... completed job in 2.667273759841919 seconds\n",
      "Transforming Categorical column.. cat31\n",
      "... completed job in 2.7339282035827637 seconds\n",
      "Transforming Categorical column.. cat32\n",
      "... completed job in 2.726888656616211 seconds\n",
      "Transforming Categorical column.. cat33\n",
      "... completed job in 2.686073064804077 seconds\n",
      "Transforming Categorical column.. cat34\n",
      "... completed job in 2.808534860610962 seconds\n",
      "Transforming Categorical column.. cat35\n",
      "... completed job in 2.729327440261841 seconds\n",
      "Transforming Categorical column.. cat36\n",
      "... completed job in 2.73415207862854 seconds\n",
      "Transforming Categorical column.. cat37\n",
      "... completed job in 2.7809441089630127 seconds\n",
      "Transforming Categorical column.. cat38\n",
      "... completed job in 2.6869759559631348 seconds\n",
      "Transforming Categorical column.. cat39\n",
      "... completed job in 2.690903425216675 seconds\n",
      "total time taken = 73.11094856262207\n",
      "Running pipeline to create sparse vectors.. \n",
      "... completed job in 6.9413135051727295 seconds\n"
     ]
    }
   ],
   "source": [
    "def transform_str_col(df, cat_name):\n",
    "\n",
    "    df_uniq_counts = df.groupBy(cat_name).count()\n",
    "\n",
    "    # get values that occur above the threshold and broadcast it\n",
    "    keep_vars = sc.broadcast(df_uniq_counts.filter(df_uniq_counts['count'] > THRESHOLD)\n",
    "                             .select(df_uniq_counts[cat_name])\n",
    "                             .rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "    # broadcast the value to replace the low occurance values\n",
    "    replace_val = sc.broadcast('stash_' + str(cat_name))\n",
    "\n",
    "    # name the new column\n",
    "    cat_t = str(cat_name) + '_t'\n",
    "\n",
    "    df = df.withColumn(cat_t, when(col(cat_name).isin(\n",
    "        keep_vars.value), col(cat_name)).otherwise(lit(replace_val.value)))\n",
    "    df = df.drop(cat_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "tot_time = 0\n",
    "for c in category_columns:\n",
    "\n",
    "    start = time.time()\n",
    "    print(f'Transforming Categorical column.. {c}')\n",
    "    df_pq = transform_str_col(df_pq, c)\n",
    "    time_taken = time.time() - start\n",
    "    print(f\"... completed job in {time_taken} seconds\")\n",
    "    tot_time += time_taken\n",
    "print(f'total time taken = {tot_time}')\n",
    "\n",
    "df_pq.cache()\n",
    "\n",
    "# transformed category column names\n",
    "cat_cols = [f'{col}_t' for col in category_columns]\n",
    "\n",
    "# indexed category column names\n",
    "cat_str_indx = [f'{col}_Indx' for col in cat_cols]\n",
    "\n",
    "# vectorized category column names\n",
    "cat_vecs = [f'{col}v' for col in category_columns]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c,\n",
    "                          outputCol=\"{0}_Indx\".format(c),\n",
    "                          handleInvalid=\"keep\")\n",
    "            for c in cat_cols]\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "                                 outputCols=cat_vecs,\n",
    "                                 dropLast=True)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numeric_columns + encoder.getOutputCols(),\n",
    "                            outputCol='features',\n",
    "                            handleInvalid='keep')\n",
    "\n",
    "label_indexer = StringIndexer(inputCol=target_col, outputCol='label')\n",
    "\n",
    "start = time.time()\n",
    "print(f'Running pipeline to create sparse vectors.. ')\n",
    "pipeline = Pipeline(\n",
    "    stages=indexers + [encoder] + [assembler] + [label_indexer])\n",
    "\n",
    "model = pipeline.fit(df_pq)\n",
    "\n",
    "transformed = model.transform(df_pq)\n",
    "\n",
    "drop_cols = cat_str_indx + cat_vecs\n",
    "final_df = transformed.drop(*drop_cols).cache()\n",
    "\n",
    "time_taken = time.time() - start\n",
    "print(f\"... completed job in {time_taken} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `final_df` data frame to parquet. This dataframe will be used in the algorithm analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing file to parquet\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('writing file to parquet')\n",
    "final_df.write.parquet('data/ohe_data.parquet', compression='snappy', mode='overwrite')\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+---+--------+--------+-----------+-----------+--------+--------+-----------+--------+--------+-----------+-----------+-----------+-----------+--------+-----------+-----------+--------+-----------+-----------+--------+-----------+--------+--------+-----------+--------+-----------+--------------------+-----+\n",
      "|  y| n1| n2| n3| n4| n5| n6| n7| n8| n9|n10|n11| n12|n13| cat14_t| cat15_t|    cat16_t|    cat17_t| cat18_t| cat19_t|    cat20_t| cat21_t| cat22_t|    cat23_t|    cat24_t|    cat25_t|    cat26_t| cat27_t|    cat28_t|    cat29_t| cat30_t|    cat31_t|    cat32_t| cat33_t|    cat34_t| cat35_t| cat36_t|    cat37_t| cat38_t|    cat39_t|            features|label|\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+---+--------+--------+-----------+-----------+--------+--------+-----------+--------+--------+-----------+-----------+-----------+-----------+--------+-----------+-----------+--------+-----------+-----------+--------+-----------+--------+--------+-----------+--------+-----------+--------------------+-----+\n",
      "|  0|  1|  1|  5|  3|375| 39| 11| 37|106|  1|  3|null|  3|5a9ed9b0|89ddfee8|stash_cat16|stash_cat17|25c83c98|3bf701e7|stash_cat20|0b153874|a73ee510|stash_cat23|stash_cat24|stash_cat25|stash_cat26|1adce6ef|stash_cat28|stash_cat29|e5ba7672|stash_cat31|stash_cat32|5840adea|stash_cat34|ad3062eb|bcdee96c|stash_cat37|f0f449dd|stash_cat39|(186,[0,1,2,3,4,5...|  0.0|\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+---+--------+--------+-----------+-----------+--------+--------+-----------+--------+--------+-----------+-----------+-----------+-----------+--------+-----------+-----------+--------+-----------+-----------+--------+-----------+--------+--------+-----------+--------+-----------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show first row of final_df\n",
    "final_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the number of distinct categories amongst the transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vars_transform = ['cat14_t', 'cat15_t', 'cat16_t', 'cat17_t', \n",
    "                  'cat18_t', 'cat19_t', 'cat20_t', 'cat21_t', 'cat22_t', 'cat23_t', 'cat23_t',\n",
    "                  'cat24_t', 'cat25_t', 'cat26_t', 'cat27_t', 'cat28_t', 'cat29_t', 'cat30_t',\n",
    "                  'cat31_t', 'cat32_t', 'cat33_t', 'cat34_t', 'cat35_t', 'cat36_t', 'cat37_t', \n",
    "                  'cat38_t', 'cat39_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in cat14_t column: 10\n",
      "Distinct values in cat15_t column: 20\n",
      "Distinct values in cat16_t column: 2\n",
      "Distinct values in cat17_t column: 7\n",
      "Distinct values in cat18_t column: 7\n",
      "Distinct values in cat19_t column: 7\n",
      "Distinct values in cat20_t column: 3\n",
      "Distinct values in cat21_t column: 8\n",
      "Distinct values in cat22_t column: 3\n",
      "Distinct values in cat23_t column: 3\n",
      "Distinct values in cat23_t column: 3\n",
      "Distinct values in cat24_t column: 6\n",
      "Distinct values in cat25_t column: 3\n",
      "Distinct values in cat26_t column: 8\n",
      "Distinct values in cat27_t column: 9\n",
      "Distinct values in cat28_t column: 2\n",
      "Distinct values in cat29_t column: 3\n",
      "Distinct values in cat30_t column: 10\n",
      "Distinct values in cat31_t column: 10\n",
      "Distinct values in cat32_t column: 3\n",
      "Distinct values in cat33_t column: 4\n",
      "Distinct values in cat34_t column: 3\n",
      "Distinct values in cat35_t column: 3\n",
      "Distinct values in cat36_t column: 9\n",
      "Distinct values in cat37_t column: 13\n",
      "Distinct values in cat38_t column: 10\n",
      "Distinct values in cat39_t column: 7\n"
     ]
    }
   ],
   "source": [
    "for v in vars_transform:\n",
    "    distinct_var_t = final_df.select(v).distinct().count()\n",
    "    print('Distinct values in {} column: {}'.format(str(v),distinct_var_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a series of checks on the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+---+--------+--------+-----------+-----------+--------+--------+-----------+--------+--------+-----------+-----------+-----------+-----------+--------+-----------+-----------+--------+-----------+-----------+--------+-----------+--------+--------+-----------+--------+-----------+\n",
      "|  y| n1| n2| n3| n4| n5| n6| n7| n8| n9|n10|n11| n12|n13| cat14_t| cat15_t|    cat16_t|    cat17_t| cat18_t| cat19_t|    cat20_t| cat21_t| cat22_t|    cat23_t|    cat24_t|    cat25_t|    cat26_t| cat27_t|    cat28_t|    cat29_t| cat30_t|    cat31_t|    cat32_t| cat33_t|    cat34_t| cat35_t| cat36_t|    cat37_t| cat38_t|    cat39_t|\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+---+--------+--------+-----------+-----------+--------+--------+-----------+--------+--------+-----------+-----------+-----------+-----------+--------+-----------+-----------+--------+-----------+-----------+--------+-----------+--------+--------+-----------+--------+-----------+\n",
      "|  0|  1|  1|  5|  3|375| 39| 11| 37|106|  1|  3|null|  3|5a9ed9b0|89ddfee8|stash_cat16|stash_cat17|25c83c98|3bf701e7|stash_cat20|0b153874|a73ee510|stash_cat23|stash_cat24|stash_cat25|stash_cat26|1adce6ef|stash_cat28|stash_cat29|e5ba7672|stash_cat31|stash_cat32|5840adea|stash_cat34|ad3062eb|bcdee96c|stash_cat37|f0f449dd|stash_cat39|\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+---+--------+--------+-----------+-----------+--------+--------+-----------+--------+--------+-----------+-----------+-----------+-----------+--------+-----------+-----------+--------+-----------+-----------+--------+-----------+--------+--------+-----------+--------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confirm if all the columns have a _t transformation\n",
    "df_pq.cache()\n",
    "df_pq.show(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the filtering of the features for a threshold number of counts, we observe that the number of distinct values has been reduced to just a few for each transformed feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct categories in the TRANSFORMED cat17 column\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print('distinct categories in the TRANSFORMED cat17 column')\n",
    "dist_cat17_t = df_pq.select(\"cat17_t\").distinct().count()\n",
    "print(dist_cat17_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 0.049354861453853205 % of the original distinct categories in the transformed cat17_t column\n"
     ]
    }
   ],
   "source": [
    "print(f'Only {dist_cat17_t/dist_cat17 * 100} % of the original distinct categories in the transformed cat17_t column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the transformed features contain just a small percent of the total number of distinct values of the original features below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for vt in vars_transform:\n",
    "#     distinct_var_t = df_pq.select(vt).distinct().count()\n",
    "#     unique_pct_original = (distinct_var_t/distinct_var) * 100\n",
    "#     print('Only {} % of the original distinct categories in the transformed {} column'.format(unique_pct_original,str(vt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new dataframe for algorithm by generating sparse vectors from the transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = ['n1_t', 'n2_t', 'n3_t', 'n4_t', 'n5_t', 'n6_t', 'n7_t',\n",
    "           'n8_t', 'n9_t', 'n10_t', 'n11_t', 'n12_t',\n",
    "           'n13_t','cat14_t', 'cat15_t', 'cat16_t', 'cat17_t',\n",
    "           'cat18_t', 'cat19_t', 'cat20_t', 'cat21_t', 'cat22_t',\n",
    "           'cat23_t', 'cat24_t', 'cat25_t', 'cat26_t', 'cat27_t',\n",
    "           'cat28_t', 'cat29_t', 'cat30_t', 'cat31_t', 'cat32_t',\n",
    "           'cat33_t', 'cat34_t', 'cat35_t', 'cat36_t', 'cat37_t',\n",
    "           'cat38_t', 'cat39_t']\n",
    "\n",
    "cat_str_indx = ['n1_t_Indx', 'n2_t_Indx', 'n3_t_Indx', 'n4_t_Indx', 'n5_t_Indx', 'n6_t_Indx', 'n7_t_Indx',\n",
    "               'n8_t_Indx', 'n9_t_Indx', 'n10_t_Indx', 'n11_t_Indx', 'n12_t_Indx',\n",
    "               'n13_t_Indx','cat14_t_Indx', 'cat15_t_Indx', 'cat16_t_Indx', 'cat17_t_Indx',\n",
    "               'cat18_t_Indx', 'cat19_t_Indx', 'cat20_t_Indx', 'cat21_t_Indx', 'cat22_t_Indx',\n",
    "               'cat23_t_Indx', 'cat24_t_Indx', 'cat25_t_Indx', 'cat26_t_Indx', 'cat27_t_Indx',\n",
    "               'cat28_t_Indx', 'cat29_t_Indx', 'cat30_t_Indx', 'cat31_t_Indx', 'cat32_t_Indx',\n",
    "               'cat33_t_Indx', 'cat34_t_Indx', 'cat35_t_Indx', 'cat36_t_Indx', 'cat37_t_Indx',\n",
    "               'cat38_t_Indx', 'cat39_t_Indx']\n",
    "\n",
    "cat_vecs = ['n1v', 'n2v', 'n3v', 'n4v', 'n5v', 'n6v', 'n7v',\n",
    "           'n8v', 'n9v', 'n10v', 'n11v', 'n12v',\n",
    "           'n13v','cat14v', 'cat15v', 'cat16v', 'cat17v',\n",
    "           'cat18v', 'cat19v', 'cat20v', 'cat21v', 'cat22v',\n",
    "           'cat23v', 'cat24v', 'cat25v', 'cat26v', 'cat27v',\n",
    "           'cat28v', 'cat29v', 'cat30v', 'cat31v', 'cat32v',\n",
    "           'cat33v', 'cat34v', 'cat35v', 'cat36v', 'cat37v',\n",
    "           'cat38v', 'cat39v']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `final_df` data frame to parquet. This dataframe will be used in the algorithm analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load files to GCP bucket and convert to RDDs for Spark analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train.txt` and `test.txt` files were downloaded to an external hard drive and subsequently loaded into a GCP bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This command streams the main data set from dropbox directly to your GCP bucket - this may take a little time (RUN THIS CELL AS IS)\n",
    "#!curl -L \"https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz\" | gsutil cp - gs://{BUCKET}/finalproject/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not run in the Docker container. This command puts a local file on GCP\n",
    "#!gsutil cp 'train.txt' gs://{BUCKET}/finalproject/train.txt\n",
    "#!gsutil cp 'train.txt' gs://danielalvarez_w261projects/finalproject/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not run in the Docker container. This command puts a local file on GCP\n",
    "#!gsutil cp 'test.txt' gs://{BUCKET}/finalproject/test.txt\n",
    "#!gsutil cp 'test.txt' gs://danielalvarez_w261projects/finalproject/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data into Spark RDDs for convenience of use later (RUN THIS CELL AS IS)\n",
    "# trainRDD = sc.textFile(f'gs://danielalvarez_w261projects/finalproject/train.txt')\n",
    "# testRDD = sc.textFile(f'gs://danielalvarez_w261projects/finalproject/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the class\n",
    "# print(type(trainRDD))\n",
    "# print(type(testRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of rows and shape of the files\n",
    "# !cat trainRDD | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to RDDs to DataFrames\n",
    "#DF = trainRDD.map(lambda x: (x.split('\\t')[0], ast.literal_eval(x.split('\\t')[1]))).toDF()\n",
    "# from pyspark.sql.types import Row\n",
    "\n",
    "# #here you are going to create a function\n",
    "# def f(x):\n",
    "#     d = {}\n",
    "#     for i in range(len(x)):\n",
    "#         d[str(i)] = x[i]\n",
    "#     return d\n",
    "\n",
    "# #Now populate that\n",
    "# df = trainRDD.map(lambda x: Row(**f(x))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
